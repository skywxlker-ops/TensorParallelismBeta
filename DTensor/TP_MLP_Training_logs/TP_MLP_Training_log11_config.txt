Configuration:
  Batch_size: 8
  context_length: 1024
  n_embd: 384
  vocab_size: 50304
  n_layers: 3
  global_batch: 65536
  grad_accum_steps: 8
  Number of parameters: 39027456
  Max Learning Rate: 0.0001
  Min Learning Rate: 1e-05
  Number of steps: 2977
  Number of warmup_steps: 297
