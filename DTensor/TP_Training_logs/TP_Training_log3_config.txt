Configuration:
  Batch_size: 8
  context_length: 1024
  n_embd: 384
  vocab_size: 50304
  n_layers: 3
  global_batch: 65536
  grad_accum_steps: 8
  Number of parameters: 11827584
  Max Learning Rate: 0.0001
  Min Learning Rate: 1e-05
  Number of steps: 902
  Number of warmup_steps: 90
