# CustomDNN: A Flexible Framework for Tensor-Parallel Models

## Overview

The `CustomDNN` module provides a C++ framework for building and training distributed neural networks with a focus on tensor parallelism. Built on top of the `DTensor` library, it offers fine-grained control over how model parameters are sharded across multiple GPUs. This makes it a powerful tool for researchers and engineers looking to implement and experiment with custom tensor-parallel model architectures directly in a high-performance environment.

The core philosophy is to provide clear, explicit control over parallelism while automating common synchronization patterns, simplifying the development of complex distributed models.

---

## Core Concepts

Before using the `CustomDNN` module, it's important to understand its foundational components:

#### 1. `DeviceMesh` and `ProcessGroupNCCL`
- **`DeviceMesh`**: A logical abstraction representing the grid or group of GPUs you are training on. It defines the world size and the rank of the current process.
- **`ProcessGroupNCCL`**: An object that manages the NCCL communicators required for collective operations (e.g., `AllReduce`) between GPUs in the mesh.

Every distributed module (`DModule`) in `CustomDNN` is tied to a `DeviceMesh` and a `ProcessGroupNCCL` to understand its place within the distributed environment.

#### 2. `ShardingType`: Explicit Parallelism Control
`ShardingType` is a configuration class that specifies how a `DTensor` is distributed across the `DeviceMesh`. It is the primary mechanism for defining the parallelism strategy.

- **`ShardingType::Replicated()`**: The default strategy. The entire tensor is copied to every GPU in the mesh.
- **`ShardingType::Shard(dim)`**: The tensor is partitioned (sharded) along a specific dimension `dim`. The size of `dim` on each GPU becomes `dim_size / world_size`.

---

## The `DLinear` Layer: The Core Building Block

`DLinear` is a distributed linear layer that implements `Y = XW + b`. Its behavior is determined by the `ShardingType` of its weight matrix.

### Parallelism Strategies for `DLinear`

#### 1. Column Parallelism (`ShardingType::Shard(1)`)
The weight matrix `W` is sharded along its output dimension (columns). This is a common strategy for the first linear layer in an MLP block.

- **How it works**: If `W` has shape `[K, N]`, each of the `P` GPUs stores a shard of shape `[K, N/P]`. The matrix multiplication `X @ W` can be computed locally on each GPU without any communication, resulting in an output that is also sharded along the same dimension.
- **Equation**: `Y = X @ [W_0, W_1, ..., W_p] = [XW_0, XW_1, ..., XW_p]`
- **Communication**: No communication is needed in the forward pass.

#### 2. Row Parallelism (`ShardingType::Shard(0)`)
The weight matrix `W` is sharded along its input dimension (rows). This is used for the second linear layer in an MLP block to produce a replicated output from a sharded input.

- **How it works**: If `W` has shape `[K, N]`, each GPU stores a shard of shape `[K/P, N]`. The input `X` must also be sharded along its last dimension. The local matrix multiplication produces a partial result. To get the final output, these partial results must be summed across all GPUs.
- **Equation**: `Y = [X_0, X_1] @ [W_0; W_1] = X_0W_0 + X_1W_1`
- **Communication**: An **AllReduce** operation is required after the local matrix multiplication. The `DLinear` layer's `forward` method **automatically handles this synchronization** when the weight is row-sharded.

---

## Workflow and User Guide

This section demonstrates the end-to-end workflow for defining and running a tensor-parallel model using the `DMLP` (Distributed Multi-Layer Perceptron) module as an example.

### The `DMLP` Architecture
The `DMLP` module is a standard two-layer block found in Transformer models, implemented with tensor parallelism:
1.  **`fc1`**: A column-parallel `DLinear` layer that expands the input dimension.
2.  **Activation**: A GeLU activation function.
3.  **`fc2`**: A row-parallel `DLinear` layer that projects the dimension back down. The AllReduce in this layer ensures the final output is replicated and correct.

### Step-by-Step Example

```cpp
#include "nn/CustomDNN.h"
#include "tensor/device_mesh.h"
#include "process_group/ProcessGroupNCCL.h"
#include <iostream>

int main() {
    // Step 1: Initialize Distributed Environment
    // (Assuming MPI is initialized and provides world_size and rank)
    int world_size, rank;
    // ... MPI_Init, MPI_Comm_size, MPI_Comm_rank ...

    auto pg = std::make_shared<ProcessGroupNCCL>(world_size, rank);
    auto mesh = std::make_shared<DeviceMesh>(pg->get_world_size(), pg->get_rank());

    // Step 2: Define the Model
    int in_features = 512;
    int hidden_features = 2048;
    auto mlp = std::make_unique<OwnTensor::dnn::DMLP>(
        in_features, 
        hidden_features, 
        in_features, 
        mesh, 
        pg
    );
    mlp->set_requires_grad(true);

    // Step 3: Create Input and Perform Forward Pass
    // Input must be replicated to be used by the first column-parallel layer
    Layout input_layout = Layout::replicated(*mesh, {1, in_features});
    DTensor input = DTensor::randn({1, in_features}, mesh, pg, input_layout);

    DTensor output = mlp->forward(input);

    // The output is now a replicated tensor, ready for the next part of the model.
    std::cout << "Rank " << rank << ": Forward pass successful." << std::endl;
    
    // Step 4: Backward Pass and Optimization
    // Let's assume 'loss' is a scalar DTensor computed from 'output'
    // loss.backward(); // This would populate gradients for all parameters

    // Create an optimizer
    auto optimizer = OwnTensor::dnn::SGD(0.01f);

    // Get model parameters
    std::vector<DTensor*> params = mlp->parameters();
    
    // Update weights
    // optimizer.step(params);

    // Zero gradients for the next iteration
    // mlp->zero_grad();

    std::cout << "Rank " << rank << ": Backward and optimization step placeholder." << std::endl;

    return 0;
}
```

---

## Pros and Cons

### Advantages: A Focus on Customizability

The `CustomDNN` framework is engineered for maximum flexibility, giving developers granular control over both model architecture and parallelism strategy.

- **Unmatched Architectural Freedom**: Developers are not confined to pre-built modules like `DMLP`. The `DLinear` layer acts as a fundamental building block that can be assembled into arbitrary network architectures. You can programmatically define networks of any depth and width, allowing for the creation of complex, non-standard model designs.

- **Granular Parallelism Control**: The true power of `CustomDNN` lies in its `ShardingType` system. This control is not just per-layer, but **per-parameter**. For each `DLinear` layer, you can independently define the sharding strategy for the `weight` and `bias`. This enables sophisticated optimization, allowing you to mix and match `Shard(0)`, `Shard(1)`, and `Replicated` strategies to perfectly balance memory usage, computational load, and communication overhead for your specific hardware and model.

- **Ideal for Research and Experimentation**: This high degree of customizability makes the framework an excellent tool for researchers. It allows for rapid prototyping and testing of novel parallelism techniques (e.g., hybrid row-column strategies) or unique model structures without being constrained by the high-level abstractions of more rigid libraries.

- **Automatic Synchronization**: While offering deep control, the framework doesn't sacrifice ease of use. It intelligently handles necessary communication, such as automatically injecting `AllReduce` operations in row-parallel layers. This lets developers focus on high-level architectural design without getting bogged down in low-level synchronization details.

- **Seamless Autograd Integration**: All custom modules are built on `DTensor` and fully support its automatic differentiation engine, ensuring that even complex, custom-designed models can be trained with ease.

### Cons
- **Limited Layer Zoo**: The framework currently provides `DLinear`, `DEmbedding`, and `DMLP`. More complex layers, such as Attention or Layer Normalization, must be implemented manually by the user.
- **CPU-Based Optimizer**: The provided `SGD` optimizer is a simple, proof-of-concept implementation that performs weight updates on the CPU. This introduces a major performance bottleneck due to frequent GPU-to-CPU data transfers. For any serious training, a custom CUDA kernel for the optimization step is essential.
- **Potential for Minor Overhead**: As a dynamic C++ library, it may have marginal performance overhead compared to a statically defined model in a more mature framework like LibTorch, due to factors like virtual function calls and dynamic memory allocations.