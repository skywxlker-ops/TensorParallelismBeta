
=== Column Parallel Embedding Test ===
World size: 2
Config: vocab=1024, n_embd=256, hidden=512
[Rank 0] Input tokens prepared.

--- Testing Column Parallel Embedding ---
Input tokens: 0 100 200 300 400 500 600 700 
[Rank 0] Calling emb_colpar.forward...
[Rank 1] Input tokens prepared.
[Rank 1] Calling emb_colpar.forward...
[Rank 0] emb_colpar.forward done.
ColPar Embedding output layout: Sharded
ColPar Local shape: [8, 128]
[Rank 0] Calling fc_rowpar.forward...
[Rank 1] emb_colpar.forward done.
[Rank 1] Calling fc_rowpar.forward...
[Rank 1] fc_rowpar.forward completed.
[Rank 0] fc_rowpar.forward completed.
FC output layout: Replicated
FC output shape: [8, 512]

--- Testing Row Parallel Embedding ---
RowPar Embedding output layout: Replicated
RowPar Local shape: [8, 256]

--- Memory Comparison ---
Row Parallel: [V/P, H] = [512, 256]
Col Parallel: [V, H/P] = [1024, 128]
Memory per GPU is equal, but Col Parallel has no AllReduce in embedding!

=== Test Complete ===
