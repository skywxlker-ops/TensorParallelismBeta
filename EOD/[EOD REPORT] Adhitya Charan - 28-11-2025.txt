[EOD REPORT] Adhitya Charan - 28-11-2025




To
	Praveen Kumar S. Kripa Shankar Person
	Cc
	Manoj Kumar Singh Navin Kumar Kiruthik Kanna Anuj Kumar 
	Bcc
	Person
	Subject
	[EOD REPORT] Adhitya Charan - 28-11-2025
	Subject : Backpropagation in Tensor Parallelism and Gradient Communication Patterns

Backpropagation in Tensor Parallelism


Today I explored how gradients flow backward through tensor-parallel distributed systems. The key challenge is understanding when and where collective communication operations (like All-Reduce) need to happen to maintain mathematical correctness across partitioned weight matrices.


Understanding the Backward Pass


In tensor parallelism, we split large weight matrices across multiple GPUs. This can be done in two ways:

1. Row-wise Parallelism - Each GPU holds different rows of the weight matrix
2. Column-wise Parallelism - Each GPU holds different columns of the weight matrix


The critical insight is that the backward pass has an inverse communication pattern compared to the forward pass.


Column-wise Parallelism Backward Pass


When weights are split column-wise:
- Forward pass: Need to All-Reduce the outputs because each GPU computes partial results
- Backward pass: Need to All-Reduce the input gradients (dL/dX)

This is because:
Y = X @ [W₀ | W₁ | W₂ | W₃]  (columns concatenated across GPUs)

Each GPU computes:
- dL/dX_local = dL/dY @ W_local^T

But the complete input gradient requires summing all local contributions:
dL/dX = dL/dY₀@W₀^T + dL/dY₁@W₁^T + dL/dY₂@W₂^T + dL/dY₃@W₃^T

So we perform All-Reduce on dL/dX to get the complete gradient.


Row-wise Parallelism Backward Pass


When weights are split row-wise:
- Forward pass: No communication needed, just compute Y = X @ W_local
- Backward pass: Need to All-Reduce the weight gradients (dL/dW)

This is because:
Each GPU processes the same input X but different weight rows. To compute the weight gradient:
dL/dW_local = X^T @ dL/dY

Since all GPUs see the full input, each computes a partial weight gradient. These need to be summed:
dL/dW = All-Reduce(dL/dW_local)


The Communication Symmetry


There's a beautiful symmetry here:
- If you All-Reduce in the forward pass → you All-Reduce input gradients in backward pass
- If you don't communicate in forward pass → you All-Reduce weight gradients in backward pass

This ensures mathematical equivalence to single-GPU training.


All-Reduce Operation


All-Reduce is a collective communication where:
1. Each GPU contributes its local tensor
2. All tensors are summed element-wise
3. The result is distributed back to all GPUs

Example with 4 GPUs:
GPU 0: [1, 2, 3]  ──┐
GPU 1: [4, 5, 6]  ──┤
GPU 2: [7, 8, 9]  ──┼──→ Sum and broadcast → Each GPU gets [12, 15, 18]
GPU 3: [10,11,12] ──┘


Why This Matters for Autograd


When integrating with automatic differentiation systems:
1. The backward() function must include the All-Reduce operation
2. The communication is part of the computational graph, not just an optimization
3. We need to preserve activations and weights for gradient computation
4. The Device Mesh structure determines which GPUs participate in each All-Reduce


Implementation Insights


From examining the codebase:
- Device mesh configuration defines row and column process groups
- NCCL handles the actual All-Reduce communication
- Layout information tracks which dimensions are sharded
- Coordinate mapping functions help determine data placement


I tested the NCCL collective operations to understand their performance characteristics. The benchmarks show how latency scales with tensor size and number of GPUs involved.


Mathematical Correctness


The chain rule must be preserved:
dL/dX = dL/dY @ dY/dX

When weights are distributed, this means:
- Each GPU computes its local contribution
- All-Reduce sums these contributions
- The result is identical to non-distributed gradient

This is not an approximation - it's mathematically exact.


Performance Considerations


Key points I learned:
1. Communication can be overlapped with computation if we pipeline the operations
2. Using non-blocking All-Reduce can hide latency
3. The ring algorithm for All-Reduce is bandwidth-optimal
4. Memory efficiency requires reusing buffers for communication


Two-Dimensional Parallelism


When combining row and column parallelism across a 2D device mesh:
- Need separate communicators for row and column groups
- Backward pass coordinates All-Reduce across the appropriate dimension
- More complex but enables scaling to many more GPUs


Visual Understanding


From the diagram I studied, the backward flow shows:

Layer Type        | Forward Comm  | Backward Comm
------------------|---------------|------------------
Column-parallel   | All-Reduce Y  | All-Reduce dL/dX
Row-parallel      | None          | All-Reduce dL/dW


Real-World Application


This knowledge is critical for:
- Training large language models that don't fit on one GPU
- Megatron-LM style tensor parallelism
- Efficient multi-GPU transformer training
- Understanding PyTorch DTensor backward propagation


Key Learnings


1. Gradient communication is mathematically required, not optional
2. Forward and backward passes have inverse communication patterns
3. All-Reduce is the core primitive for gradient aggregation
4. Device mesh structure determines communication topology
5. Correct gradient flow requires careful autograd integration


Challenges Identified


- Debugging distributed gradients across GPUs is complex
- Need numerical gradient checking to validate correctness
- Memory management during backward pass with large activations
- Timing the All-Reduce to overlap with computation
- Handling edge cases with irregular tensor sizes


Next Steps


Tomorrow I plan to:
- Implement custom autograd functions with embedded All-Reduce
- Add gradient validation tests comparing single-GPU vs distributed
- Benchmark backward pass communication overhead
- Explore gradient checkpointing with tensor parallelism
- Study how PyTorch DTensor implements backward operators


This deep dive into tensor parallel backpropagation has given me a solid foundation for implementing distributed training systems correctly.


Adhitya Charan


Regards
