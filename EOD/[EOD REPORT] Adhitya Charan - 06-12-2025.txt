[EOD REPORT] Adhitya Charan - 06-12-2025

Subject: KV Caching and Scaled Dot Product Attention

KV Caching Architecture

Today I went through how KV caching works in Transformer attention layers. The idea is that during autoregressive generation, instead of recomputing all the key and value vectors from scratch at each step, we store them in fixed-size buffers and just compute the query for the new token.

The KVCache module maintains two buffers: k_cache and v_cache, each with shape (max_batch_size, n_heads, max_seq_length, head_dim). These are initialized during model setup for every TransformerBlock's attention layer. The buffers are pre-allocated to the maximum sequence length, so there's no memory allocation during generation.


The Update Mechanism

During each forward pass, the attention layer computes new k and v vectors from the current input tokens and writes them into the cache at specific positions indicated by the input_pos tensor. The operation is just k_cache[:, :, input_pos] = k_val and similarly for v_cache.

The input_pos tensor contains the sequence indices where the new tokens should be stored. For autoregressive generation, if you're generating the 10th token, input_pos would be [9], and the new k and v vectors get written to position 9 in the cache. The function then returns the complete cached history so the current query can attend to everything.


Why Query Isn't Cached

Query doesn't need to be stored. Query is about the current token asking "what should I pay attention to?" - it's used once and discarded. The key vectors represent "what information do I have?" and values represent "what content should I contribute if attended to?". The query is computed fresh each time from the current input, used to compute attention with the cached keys, and then not needed anymore.


The Attention Score Matrix

The Q·K^T attention score matrix has shape (B, H, T, T) for the full sequence case, but it's never stored in the cache. It's a temporary computation that exists only during the attention calculation for the current step.

The attention score matrix grows quadratically with sequence length (T×T), while storing just K and V grows linearly (2×T×D_h). By storing only K and V and recomputing the attention scores each time, we trade a small amount of computation for memory savings. This is why KV caching matters for long-context generation.


Causal Masking

The mask is precomputed as a lower triangular boolean matrix using torch.tril(torch.ones(max_seq_length, max_seq_length)). During the forward pass, the appropriate slice is selected based on input_pos.

The boolean mask gets converted to an additive float mask where masked positions become negative infinity. When this is added to the attention scores and passed through softmax, e^(-∞) approaches zero. So the masked connections get zero attention weight.

The memory for the attention score tensor doesn't shrink when masking happens - the tensor maintains its full dimensions. But the values at masked positions become -∞ before softmax and then 0.0 after softmax. It's a mathematical zeroing rather than a memory-level one.


Flash Attention and Fused Kernels

The default mathematical backend uses at::matmul with explicit transposition, but modern systems prefer fused kernels like Flash Attention. The dispatch mechanism selects between flash_attention, efficient_attention, cudnn_attention, or the math fallback based on what's available on the device.

Flash Attention integrates the Q·K^T multiplication, scaling, masking, softmax, and multiplication by V into a single operation. It never materializes the full attention matrix, instead computing it block by block. This matters for long sequences where the T×T matrix would be too large.


Dynamic Shapes During Generation

When processing tokens sequentially with KV caching, the query has sequence length 1 (the current token), while the key has sequence length T (all cached tokens). The matrix multiplication Q·K^T gives shape (B, H, 1, T) - essentially extracting just the last row of the conceptual T×T attention matrix.

Instead of computing a full T×T attention matrix and applying the causal mask to zero out the upper triangle, we compute just the 1×T row we actually need for the current token.


Backpropagation

The activation tensors (q, k, v) receive gradients, but they're not "updated" themselves. The gradients flow through them to update the projection weights (W_Q, W_K, W_V) which are the actual trainable parameters.

The flash_attention_backward_kernel computes grad_q, grad_k, and grad_v. These gradients are propagated back to update the qkv_projection weights. The intermediate Q·K^T matrix also participates in the computation graph, but it's temporary - after the forward pass, if attention weights aren't explicitly needed, the tensor is cleared.


Key Takeaways

KV caching is about trading O(T) computation for O(T) storage rather than O(T²) recomputation. The attention mechanism needs all previous keys and values to compute the current output, but not the previous attention scores or queries.

The causal mask is applied at the score level (Q·K^T), not the weight level after softmax. You modify the scores once and the softmax naturally produces zeros where needed.

The architecture handles both batch processing of sequences (training) and iterative token-by-token generation (inference) with the same code, just by varying what gets passed to input_pos.


Adhitya Charan

Regards
