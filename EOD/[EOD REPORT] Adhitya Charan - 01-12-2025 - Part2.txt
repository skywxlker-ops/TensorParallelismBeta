[EOD REPORT] Adhitya Charan - 01-12-2025




To
	Praveen Kumar S. Kripa Shankar Person
	Cc
	Manoj Kumar SinghNavin Kumar Kiruthik Kanna Anuj Kumar 
	Bcc
	Person
	Subject
	[EOD REPORT] Adhitya Charan - 01-12-2025 (Part 2)
	Subject : All-to-All Personalized Communication Patterns and Algorithm Analysis

All-to-All Personalized Communication


Today I spent considerable time studying all-to-all personalized communication patterns and their implementation across different network topologies. This is quite relevant for our DTensor work because these patterns form the foundation of many collective operations we'll need, especially for operations like tensor redistribution and multi-dimensional sharding patterns.


  

Understanding All-to-All Personalized Communication


The fundamental concept is that in all-to-all personalized communication, each node sends a distinct message to every other node. This is different from all-to-all broadcast where every node sends the same message to everyone. The personalized variant is more complex because each of the p nodes needs to send p-1 unique messages, one to each other node.

What makes this particularly interesting is that this operation is mathematically equivalent to transposing a distributed matrix. If you have an n x n matrix distributed row-wise across p processors, and you want to transpose it, you're essentially doing an all-to-all personalized communication. Each processor initially has a row of the matrix, and after transposition, it needs to have a column. The element at position [i, j] starts on processor i but needs to go to processor j. This exact pattern - where every processor sends distinct data to every other processor - is all-to-all personalized communication.


  

Matrix Transpose as a Concrete Example


The matrix transpose example really crystallizes the idea. Consider a 4x4 matrix distributed across 4 processors where each processor holds one row. Processor 0 has row 0, processor 1 has row 1, and so on. After transposition, processor 0 should have column 0, which means it needs element [0,0] from itself, element [1,0] from processor 1, element [2,0] from processor 2, and element [3,0] from processor 3.

From processor 0's perspective, it needs to send element [0,1] to processor 1, element [0,2] to processor 2, and element [0,3] to processor 3. Every other processor is doing similarly - sending different elements to different destinations. The communication pattern that emerges is all-to-all personalized.

When you scale this up to use p processors for an n x n matrix where p ≤ n, each processor initially holds n/p rows, which is n²/p elements total. The transpose now involves all-to-all personalized communication of matrix blocks of size n/p x n/p instead of individual elements. This blocking reduces the number of messages and improves efficiency.


  

Implementation on Ring Topology


The ring topology implementation is quite elegant. Each node has p-1 pieces of data to send, each of size m. The algorithm works in p-1 steps. In the first step, each node consolidates all its outgoing data into one large message of size m(p-1) and sends it to its neighbor. All nodes send in the same direction, either all clockwise or all counterclockwise.

When a node receives this consolidated message, it extracts the m-word packet that's meant for itself and forwards the remaining (p-2) packets to the next node. In each successive step, the message size decreases by m words because one more destination has received its data. After p-1 steps, every node has received its personalized message from every other node.

The key insight here is that the average distance a packet travels is p/2 hops. Since there are p nodes each sending m(p-1) words, the total traffic on the network is m(p-1) × p/2 × p. With p links in the ring, this gives a communication time of tw × m(p-1)p/2, which matches the algorithmic cost. This means the ring algorithm is optimal - there's no better way to route the messages given the topology.


  

Mesh Topology Strategy


The mesh implementation uses a two-phase approach that's quite clever. For a √p x √p mesh, each node initially has p messages of size m. The algorithm first groups messages by column destinations. So on a 3x3 mesh, node 0 would group its messages into three bundles - one for column 0 (nodes 0, 3, 6), one for column 1 (nodes 1, 4, 7), and one for column 2 (nodes 2, 5, 8).

In the first phase, all-to-all personalized communication happens independently within each row using these clustered messages of size m√p. By the end of this phase, each node has received all messages destined for its column, but they're not at the right rows yet.

Before the second phase, messages are regrouped according to row destinations. Then similar communication happens in each column. After this second phase, every node has received its personalized message from every other node.

The cost analysis shows that each phase takes time proportional to ts√p + twm√p(√p - 1), so the total time is roughly 2ts√p + 2twmp(1 - 1/√p). Compared to the ring which is O(p), the mesh achieves O(√p) which is much better for large p. This √p advantage comes from having two dimensions to work with instead of just one.


  

Hypercube - Two Different Approaches


The hypercube case is fascinating because there are two distinct algorithms with different tradeoffs. The first algorithm extends the mesh approach to log p dimensions. In each of log p steps, nodes exchange data across different hypercube dimensions. In each step, every node sends p/2 messages consolidated into one, for a total of mp/2 words.

The total communication time for this first algorithm is ts log p + twmp(p-1)/2. However, this algorithm requires local data rearrangement before each communication step to ensure messages destined for the same subcube are contiguous in memory. This adds trmp log p time for memory operations.

What's interesting is that this algorithm is not optimal for the hypercube. The average distance between nodes on a hypercube is (log p)/2, and with p(p-1) total pairwise exchanges of m words, the theoretical lower bound is much lower than what this algorithm achieves.


  

The Optimal Hypercube Algorithm


The second hypercube algorithm is beautifully simple and optimal. Since every pair of nodes needs to exchange data anyway, why not have them communicate directly? The algorithm just runs p-1 communication steps where in step j, node i exchanges data with node (i XOR j).

The XOR pattern is brilliant because it naturally creates a congestion-free schedule. In step 1, nodes communicate with neighbors differing in the least significant bit. In step 2, they communicate with neighbors differing in the second bit, and so on. Because of the hypercube's structure, these XOR patterns ensure no link carries conflicting traffic in the same direction.

The routing uses E-cube routing where messages travel in dimensions corresponding to the nonzero bits in (i XOR j), sorted in ascending order. This gives a shortest path and avoids congestion. The total communication time is ts(p-1) + twm(p-1), which matches the theoretical lower bound.

The tradeoff between the two algorithms depends on message size. For small messages where ts dominates, the first algorithm with log p startups might be better than the second algorithm with p-1 startups. But for larger messages where tw dominates, the second algorithm wins because it only transfers twmp(p-1) vs twmp(p-1)/2 in the first algorithm... wait, actually the first algorithm has a factor of p/2 in the tw term, so it's twmp(p-1)/2 vs twm(p-1). For large p, the first algorithm has higher tw cost by a factor of p/2, so the second algorithm is indeed better for large messages.


  

Relevance to DTensor Operations


Understanding these all-to-all personalized communication patterns is directly applicable to DTensor operations. Whenever we need to change sharding strategies - for instance, converting from row-wise sharding to column-wise sharding, or going from 1D sharding to 2D sharding - we're essentially doing a distributed transpose which requires all-to-all personalized communication.

The mesh algorithm's two-phase approach is particularly relevant for 2D device meshes which is exactly what we're using in our DTensor implementation. When we have GPUs arranged in a 2D grid and need to redistribute tensors, we can use the same row-then-column communication pattern.

The hypercube algorithms are interesting for thinking about how to optimize communication when we have more complex network topologies. Many modern GPU clusters have hierarchical topologies - NVLink within a node, PCIe between nodes in a rack, and network fabric between racks. The hypercube dimension-by-dimension approach could map well to these hierarchical levels.


  

Cost Analysis Insights


What struck me from studying these algorithms is how critical the network topology is to communication cost. On a ring with p nodes, the cost grows as O(p²) for the tw term because messages travel O(p) hops on average. On a mesh, it's O(p^1.5) because of the √p advantage from two dimensions. On a hypercube with the optimal algorithm, it's O(p) which is the best possible since every node must send and receive O(p) total data.

The startup time ts vs transfer time tw tradeoff also appears repeatedly. Algorithms that minimize the number of communication steps (reduce ts overhead) often have to send larger consolidated messages, while algorithms that allow more steps can send smaller messages. The optimal choice depends on message size m and the ratio ts/tw for the particular hardware.

Another important consideration is local memory operations. The hypercube and mesh algorithms both require data rearrangement between communication phases. This takes time tr per word for read-write operations. For modern systems, tr is much smaller than tw, so communication dominates. But for very small messages or very fast networks, the local rearrangement could become significant.


  

Applications Beyond Matrix Transpose


The material mentioned that all-to-all personalized communication is used in fast Fourier transform, matrix transpose, sample sort, and parallel database join operations. This makes sense because all these operations involve global data redistribution where elements need to move to different processors based on some key or index.

For FFTs, the butterfly communication pattern requires data exchanges between specific processor pairs in each stage. For sample sort, after determining splitters, each processor needs to send different portions of its data to different processors based on which bucket the elements fall into. For database joins, tuples need to be redistributed based on join keys. All of these map naturally to all-to-all personalized communication.


  

Next Steps and Implementation


For our DTensor implementation, I'm thinking we should implement support for layout changes that require all-to-all personalized communication. Currently we can do operations like all-reduce where all processors get the same result, but we don't have good support for redistributing sharded tensors between different sharding patterns.

Implementing the mesh two-phase algorithm would be a good starting point since we're using 2D device meshes. We'd need to add NCCL group calls to do row-wise and column-wise communication phases. The data rearrangement between phases could use temporary buffers from our caching allocator.

It would also be interesting to benchmark the different strategies to see which performs better on our actual hardware. The theoretical analysis assumes uniform link bandwidth and latency, but in practice NVLink, PCIe, and network fabric have very different characteristics. Measuring actual performance could reveal which strategy works best for our specific use cases.


  

Adhitya Charan


Regards
