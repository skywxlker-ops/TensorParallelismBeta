[EOD REPORT] Adhitya Charan - 28-11-2025




To
	Praveen Kumar S. Kripa Shankar Person
	Cc
	Manoj Kumar SinghNavin Kumar Kiruthik Kanna Anuj Kumar 
	Bcc
	Person
	Subject
	[EOD REPORT] Adhitya Charan - 20-11-2025
	Subject : AlltoAll Operation in MOE and CollCommOperations Benchmarking
All to All Operation 


All to All is another collective communication operation
In it there are a set of processing elements. Among a set of processing elements (nodes) each node has distinct (personalized) data items destined for each of the other nodes.


The all-to-all operation accomplishes this total data exchange among the set of nodes, such that each node ends up having an individual data item from each of the other nodes.


  



All to All is a Transpose operation on each sub element in the data in each GPU vs the GPU in which the data resides on. 


  





This kind of data transfer is useful while working on Mixture of experts because the Experts are located in each GPU separately but the tokens are available in all the GPUs 


Once they are allocated in each GPU based on the router. Then they all need to be reallocated such that they are gathered together in each GPU based on what expert it is assigned to ( whichever GPU has that particular expert ) rather than the GPU in which it was found in, so they need to perform an All to All collective communication operation again. 


And once the tokens are passed through the expert block, they need to be reallocated back again to be gathered together based on GPUs in which they came from again. 
So there needs to be an All to All operation performed again. 


  



This way there is a Transpose operation that is performed on the matrix denoting the GPU in which the token needs to reside  vs  Expert that the token needs to go to. 


  



I also looked into refining the benchmarking of the Collective Communication Operations to stress test how much memory each operation could allocate and transfer and with how much latency. 


I also made the All Reduce Operation run by combining the Reduce Scatter and All Gather operations together. 


  





I was able to make all the operations run until 10.75 GB but once it crossed that the memory started to go out of bound. 




  











  



AllReduce


  



Reduce


  



Broadcast


  



AllGather
  



Gather


  



ReduceScatter
  



Scatter 


  



AlltoAll
  



ReduceScatter + AllGather ( AllReduce )


  



I am planning to look into integrating autograd into DTensor tomorrow. 


Adhitya Charan


Regards