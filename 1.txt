ðŸ“Š CUDA GPU Kernel Summary
Kernel	Time %	Total Time	Instances	Avg Time
batched_matmul_kernel	93.3%	1.19s	420	2.84ms
ncclDevKernel_AllReduce	6.7%	85.5ms	210	0.41ms
ðŸ”‘ Key Findings
1. Compute-Dominated Workload (93% MatMul)
The batched_matmul_kernel dominates execution time
420 kernel calls = 210 per GPU Ã— 2 matmuls Ã— 100 iterations âœ“
Average: 2.84ms per matmul (could be optimized with cuBLAS)
2. Communication Overhead is Low (6.7%)
NCCL AllReduce only takes 6.7% of GPU time
This is a good compute-to-communication ratio for tensor parallelism
3. Memory Operations
Operation	Total	Count	Avg Size
Host-to-Device Copy	538 MB	2,566	0.21 MB
Memset	868 MB	464	1.87 MB
ðŸ’¡ Optimization Opportunities
Use cuBLAS GEMM instead of custom matmul kernel for better performance
Reduce H2D transfers - the 2,566 small copies suggest inefficient data handling
Batch operations to reduce kernel launch overhead
Throughput achieved: ~1.39 TFLOPS
Theoretical peak on 2 GPUs: Much higher - indicates room for optimization!