1. Workload Imbalance (The "Data Skew")
The logs show a significant imbalance in the number of tokens processed by each rank during the Embedding forward pass:

Rank 0: 3942 tokens
Rank 1: 154 tokens
Total: 4096 tokens ($B=4, T=1024$)
In Tensor Parallelism, each rank only handles the embedding rows that fall within its vocabulary shard. Because the input batch consists of common tokens (likely at the start of the vocabulary), Rank 0 is doing nearly all the work, while Rank 1 is processing a very small subset.

so why is the split unbalanced??

lets first fix that

is there a problem with dembedding then?

lets pinpoint first then only we can move further



2. CPU-Side Processing Overhead
The DEmbedding::forward implementation performs a CPU-side loop over all $4096$ tokens in every iteration to generate masks and local indices.

for (int64_t i = 0; i < batch_size; ++i) {
    // ... CPU checks every single token ID ...
}
This $O(B \times T)$ loop, followed by host-to-device memory copies (device::copy_memory), introduces a "sequential" bottleneck that must be completed before any GPU computation can even begin.

is there any other way to do this? like moving it to gpu maybe?


3. The "Stoppage" Point: Synchronization Hang
The logs show that:

Both ranks passed the Embedding Sync.
Rank 0 proceeded into the first MLP block and finished the first matmul.
Rank 0 is now waiting for an AllReduce (partial sum reduction) inside 
_row_parallel_matmul
.
Rank 1 has not yet reached that matmul.
Rank 0 is effectively "hanging" at the collective communication barrier because Rank 1 is lagging behind, possibly stuck in one of the intermediate steps:

pos_emb = wpe->forward(pos)
tok_emb + pos_emb (GPU Addition)
LayerNorm
 computation

can we use nsights to see whasts happening inside gpu1? maybe we can pinpoint the problem them ??
for the time being, lets remove pos_emb from the forward pass to check whether it is the actual culprit

4. Impact of Debug Mode
Using CUDA_LAUNCH_BLOCKING=1 forces the CPU to wait for the GPU after every single operation. While this makes it easier to find where a crash happens, it also:

Prevents CPU/GPU overlap (the CPU cannot prepare the next batch while the GPU is working).
Prevents Rank overlap (Rank 1 cannot start its next task until Rank 0 finishes its kernel).
Amplifies the impact of any small delay into a full secondary stoppage.




Okay first lets try to remove debug mode
I dont need the debug script anymore

We have more or less figured out the issues

lets go step by step

1. Workload imbalance - make sure the split is balanced, both ranks should have equal number of tokens

2. The sync hang - i dont know how ot approach rn , i am open for suggestions tho

hopefully the other bugs will fix by themselves  


next steps - 

1. Performance Restoration
Remove Debug Mode: Stop using CUDA_LAUNCH_BLOCKING=1. This allows the CPU to prepare the next batch while the GPU works and enables ranks to overlap their communication and computation.

2. Workload Balancing (Fixing Data Skew)
Synthetic Data Injection: Modify the training loop in 
gpt2_test.cpp
 to use random tokens for the first few steps. This ensures both ranks receive a balanced number of tokens in the Vocab-Parallel Embedding layer, preventing one rank from idling.

3. Hang Isolation
Disable Position Embeddings: Temporarily comment out the addition of position embeddings in 
gpt2_test.cpp
 to verify if the hang occurs during that specific kernel or subsequent 
LayerNorm
/
MLP
 blocks.
Granular Tracing: Add start/end logs for every NCCL collective in 
dtensor.cpp
 to pinpoint the exact deadlock location.

will try to wrap this up tomorrow
