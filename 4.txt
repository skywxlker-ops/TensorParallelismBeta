cachingAllocator.h

#pragma once
#include <cstddef>
#include <unordered_map>
#include <set>
#include <mutex>
#include <atomic>
#include <cuda_runtime.h>
#include <iostream>
#include <vector>

enum class Device { CPU, CUDA };
enum class PoolType { SMALL, LARGE };

struct Block {
    void* addr;
    size_t size;
    bool active;
    PoolType pool_type;
    Block* prev;
    Block* next;
    cudaStream_t stream;

    Block(void* a = nullptr, size_t s = 0, PoolType p = PoolType::SMALL, cudaStream_t str = 0)
        : addr(a), size(s), active(false), pool_type(p),
          prev(nullptr), next(nullptr), stream(str) {}
};

static constexpr size_t ONE_MB = 1024UL * 1024UL;
static constexpr size_t ALIGNMENT = 512;
static constexpr size_t MIN_SPLIT = 512;

// CUDA stream hashing helpers
struct StreamHasher {
    std::size_t operator()(const cudaStream_t &s) const noexcept {
        return std::hash<std::uintptr_t>()(reinterpret_cast<std::uintptr_t>(s));
    }
};
struct StreamEq {
    bool operator()(const cudaStream_t &a, const cudaStream_t &b) const noexcept {
        return reinterpret_cast<std::uintptr_t>(a) == reinterpret_cast<std::uintptr_t>(b);
    }
};

// Block size comparator (for best-fit multiset)
struct CompareBySize {
    bool operator()(const Block* a, const Block* b) const {
        if (a->size != b->size) return a->size < b->size;
        return std::uintptr_t(a->addr) < std::uintptr_t(b->addr);
    }
};

struct StreamInternal {
    std::multiset<Block*, CompareBySize> small_pool;
    std::multiset<Block*, CompareBySize> large_pool;
    std::vector<Block*> all_blocks;
};

class CachingAllocator {
public:
    CachingAllocator();
    ~CachingAllocator();

    Block* allocateMemory(size_t size, cudaStream_t stream);
    void freeMemory(Block* block);
    Block* requestMemory(size_t size, PoolType pool_type, cudaStream_t stream);
    void emptyCache(Device device = Device::CPU);

    size_t memoryAllocated() const;
    size_t memoryFree() const;
    void printStats() const;
    bool isAllocated(Block* block) const;

private:
    mutable std::mutex mutex_;
    std::unordered_map<cudaStream_t, StreamInternal, StreamHasher, StreamEq> stream_to_cache_;
    std::atomic<size_t> total_allocated_;
    std::atomic<size_t> total_free_;

    size_t roundMemorySize(size_t size) const;
    PoolType selectPoolType(size_t size) const;
    Block* allocNewBlock(size_t size, cudaStream_t stream, PoolType pool_type);
    Block* findBestFit(std::multiset<Block*, CompareBySize>& pool, size_t size);
    void mergeAdjacent(Block* block, StreamInternal& internal);
};

cachingAllocator.cpp

#include "cachingAllocator.hpp"
#include <algorithm>
#include <cassert>
#include <sstream>

CachingAllocator::CachingAllocator()
    : total_allocated_(0), total_free_(0) {}

CachingAllocator::~CachingAllocator() {
    try {
        emptyCache(Device::CUDA);
    } catch (...) {
        // destructor must not throw
    }
}

size_t CachingAllocator::roundMemorySize(size_t size) const {
    if (size == 0) return 0;
    if (size < ONE_MB) {
        size_t rem = size % ALIGNMENT;
        return rem ? size + (ALIGNMENT - rem) : size;
    } else {
        size_t rem = size % ONE_MB;
        return rem ? size + (ONE_MB - rem) : size;
    }
}

PoolType CachingAllocator::selectPoolType(size_t size) const {
    return (size < ONE_MB) ? PoolType::SMALL : PoolType::LARGE;
}

Block* CachingAllocator::allocNewBlock(size_t size, cudaStream_t stream, PoolType pool_type) {
    void* dev_ptr = nullptr;
    size_t rsize = roundMemorySize(size);
    cudaError_t err = cudaMalloc(&dev_ptr, rsize);
    if (err != cudaSuccess) {
        std::ostringstream os;
        os << "cudaMalloc failed (" << cudaGetErrorString(err) << ") for size " << rsize;
        throw std::runtime_error(os.str());
    }

    Block* b = new Block(dev_ptr, rsize, pool_type, stream);
    auto& internal = stream_to_cache_[stream];
    internal.all_blocks.push_back(b);
    total_allocated_ += rsize;
    return b;
}

Block* CachingAllocator::findBestFit(std::multiset<Block*, CompareBySize>& pool, size_t size) {
    if (pool.empty()) return nullptr;
    Block fake(nullptr, size, PoolType::SMALL, 0);
    auto it = pool.lower_bound(&fake);
    return (it == pool.end()) ? nullptr : *it;
}

Block* CachingAllocator::allocateMemory(size_t size, cudaStream_t stream) {
    if (size == 0) return nullptr;

    size_t rsize = roundMemorySize(size);
    PoolType pool_type = selectPoolType(rsize);

    std::lock_guard<std::mutex> lk(mutex_);
    auto& internal = stream_to_cache_[stream];
    auto& pool = (pool_type == PoolType::SMALL) ? internal.small_pool : internal.large_pool;

    Block* candidate = findBestFit(pool, rsize);
    if (candidate) {
        pool.erase(pool.find(candidate));
        candidate->active = true;
        total_free_ -= candidate->size;

        // Optional split for large blocks
        if (candidate->size > rsize + MIN_SPLIT) {
            size_t rem_size = candidate->size - rsize;
            void* rem_addr = reinterpret_cast<void*>(reinterpret_cast<char*>(candidate->addr) + rsize);
            candidate->size = rsize;
            Block* remainder = new Block(rem_addr, rem_size, pool_type, stream);
            internal.all_blocks.push_back(remainder);
            pool.insert(remainder);
        }

        return candidate;
    }

    // No free block â€” allocate fresh
    Block* nb = allocNewBlock(rsize, stream, pool_type);
    nb->active = true;
    return nb;
}

void CachingAllocator::freeMemory(Block* block) {
    if (!block) return;

    std::lock_guard<std::mutex> lk(mutex_);
    if (!block->active) {
        std::cerr << "[Allocator Warning] Double free on block " << block->addr << std::endl;
        return;
    }

    block->active = false;
    total_free_ += block->size;

    auto it = stream_to_cache_.find(block->stream);
    if (it == stream_to_cache_.end()) return;
    StreamInternal& internal = it->second;
    auto& pool = (block->pool_type == PoolType::SMALL) ? internal.small_pool : internal.large_pool;

    mergeAdjacent(block, internal);
    pool.insert(block);
}

void CachingAllocator::mergeAdjacent(Block* block, StreamInternal& internal) {
    // Merge previous
    if (block->prev && !block->prev->active) {
        block->prev->size += block->size;
        block->prev->next = block->next;
        if (block->next) block->next->prev = block->prev;
        delete block;
        block = block->prev;
    }

    // Merge next
    if (block->next && !block->next->active) {
        block->size += block->next->size;
        Block* nxt = block->next;
        block->next = nxt->next;
        if (nxt->next) nxt->next->prev = block;
        delete nxt;
    }
}

void CachingAllocator::emptyCache(Device device) {
    if (device == Device::CPU) return;

    std::lock_guard<std::mutex> lk(mutex_);
    for (auto& entry : stream_to_cache_) {
        StreamInternal& internal = entry.second;
        for (Block* b : internal.all_blocks) {
            if (b->addr) cudaFree(b->addr);
            delete b;
        }
        internal.all_blocks.clear();
        internal.small_pool.clear();
        internal.large_pool.clear();
    }
    stream_to_cache_.clear();
    total_allocated_ = 0;
    total_free_ = 0;
}

size_t CachingAllocator::memoryAllocated() const {
    return total_allocated_.load(std::memory_order_relaxed);
}

size_t CachingAllocator::memoryFree() const {
    return total_free_.load(std::memory_order_relaxed);
}

bool CachingAllocator::isAllocated(Block* block) const {
    return block && block->active;
}

void CachingAllocator::printStats() const {
    std::lock_guard<std::mutex> lk(mutex_);
    std::cout << "\n[GPU CachingAllocator Stats]\n";
    std::cout << "  Total Allocated: " << memoryAllocated() << " bytes\n";
    std::cout << "  Total Free:      " << memoryFree() << " bytes\n";
    std::cout << "  Streams Cached:  " << stream_to_cache_.size() << "\n";
    for (auto& kv : stream_to_cache_) {
        const StreamInternal& s = kv.second;
        std::cout << "    Stream[" << reinterpret_cast<std::uintptr_t>(kv.first)
                  << "]: small=" << s.small_pool.size()
                  << ", large=" << s.large_pool.size()
                  << ", total_blocks=" << s.all_blocks.size() << "\n";
    }
}
