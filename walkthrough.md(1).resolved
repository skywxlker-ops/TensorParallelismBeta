# CUDA Hang Fixes — Walkthrough

## Bugs Fixed

### 1. `CachingCUDAAllocator::deallocate` crash

[deallocate()](file:///home/blu-bridge25/Study/Code/TensorParallelismBeta/DTensor/Tensor-Implementations/src/device/CudaCachingAllocator.cpp#160-216) used `cudaGetDevice()` to find the device pool. In multi-GPU, the active device at deallocation can differ from allocation. **Fix**: search all device pools.

```diff:CudaCachingAllocator.cpp
#include "device/CachingCudaAllocator.h"
#include "device/SizeClass.h"
#include "device/AllocationTracker.h"
#include "device/DeviceCore.h"
#include <iostream>
#include <algorithm>

namespace OwnTensor
{
    CachingCUDAAllocator& CachingCUDAAllocator::instance()
    {
        static CachingCUDAAllocator allocator;
        return allocator;
    }

    CachingCUDAAllocator::CachingCUDAAllocator()
    {
        int device_count = 0;
        cudaGetDeviceCount(&device_count);
        device_pools_.resize(device_count);
    }

    CachingCUDAAllocator::~CachingCUDAAllocator()
    {
        empty_cache();
    }

    void* CachingCUDAAllocator::allocate(size_t bytes)
    {
        cudaStream_t stream = cuda::getCurrentStream();
        return allocate(bytes, stream);
    }

    void CachingCUDAAllocator::trim_to(size_t target_bytes)
    {
        for (DevicePools& pools : device_pools_)
        {
            std::lock_guard<std::mutex> lock(*pools.mtx);
            trim_pool(pools.small_pool, target_bytes / 2);
            trim_pool(pools.large_pool, target_bytes / 2);
        }
    }

    void CachingCUDAAllocator::trim_pool(BlockPool& pool, size_t target_bytes) {
    // ASSUMES LOCK IS HELD

    // Use a while loop with a safe forward-iterator check
    while (pool.total_cached > target_bytes && !pool.free_blocks.empty()) {
        
        // Always take the largest block to satisfy OOM quickly
        // In std::set, the largest element is at the end
        auto it = std::prev(pool.free_blocks.end());
        Block* block = *it;

        // CRITICAL: Update pointers and stats BEFORE the slow cudaFree
        pool.total_cached -= block->size;
        // pool.allocated_blocks.erase(block->ptr); // Not in allocated blocks if it's in free_blocks
        pool.free_blocks.erase(it);

        // LOCK #2: Stats update (must be very brief)
        {
            std::lock_guard<std::mutex> s_lock(stats_mutex_);
            total_frees_++;
        }

        // PHYSICAL FREE
        // We do this while still holding pool.mtx to prevent 
        // another thread from getting the same pointer from the driver
        cudaStreamSynchronize(block->stream);
        cudaFree(block->ptr); 
        
        assert(block != nullptr);
        delete block;
    }
}

    void* CachingCUDAAllocator::allocate(size_t bytes, cudaStream_t stream)
    {
        if (bytes == 0) return nullptr;

        int device;
        cudaGetDevice(&device);

        size_t alloc_size = SizeClass::round_size(bytes);

        BlockPool& preferred_pool = get_pool(alloc_size, device);
        BlockPool* actual_pool_ptr = &preferred_pool;

        Block* block = nullptr;
        {
            std::lock_guard<std::mutex> lock(*device_pools_[device].mtx);
            
            // Try preferred pool first
            block = preferred_pool.find_free_block(alloc_size, stream);
            
            // If missed in small pool, try large pool as fallback
            if (!block && SizeClass::is_small(alloc_size)) {
                block = device_pools_[device].large_pool.find_free_block(alloc_size, stream);
                if (block) {
                    // Pull from large pool!
                    actual_pool_ptr = &device_pools_[device].large_pool;
                }
            }

            if (block)
            {
                {
                    std::lock_guard<std::mutex> s_lock(stats_mutex_);
                    cache_hits_ += 1;
                }
                actual_pool_ptr->total_cached -= block->size;

                if (block->size >= alloc_size + SizeClass::kSmallSize)
                {
                    block = try_split(block, alloc_size);
                }
            }
        }

        if (block) {
            ensure_stream_safety(block, stream);
        }

        // Cache miss - need fresh CUDA allocation
        if (!block) {
            cache_misses_++;
            block = cuda_alloc(alloc_size, device, stream);
            if (!block) {
                // Log to system stderr directly to bypass potential stream hangs
                fprintf(stderr, "[ALLOCATOR] Hard OOM. Trimming cache...\n");
                num_ooms_++;
                
                this->trim_to(0); 
                
                block = cuda_alloc(alloc_size, device, stream);
                if (!block) {
                    fprintf(stderr, "[ALLOCATOR] FATAL: Still OOM after trim.\n");
                    throw std::runtime_error("CUDA OOM");
                }
                num_alloc_retries_++;
            }
        }


        block->allocated = true;
        block->req_size = bytes;
        block->stream = stream;

        {
            std::lock_guard<std::mutex> lock(*device_pools_[device].mtx);
            actual_pool_ptr->allocated_blocks[block->ptr] = block;
        }

        AllocationTracker::instance().on_alloc(block->ptr, bytes, device);

        total_allocs_++;
        return block->ptr;
    }

    void CachingCUDAAllocator::deallocate(void* ptr)
    {
        if (!ptr) return;

        int device;
        cudaGetDevice(&device);

        Block* block = nullptr;
        BlockPool* pool_ptr = nullptr;

        {
            std::lock_guard<std::mutex> lock(*device_pools_[device].mtx);
            for (BlockPool* pool :
                { &device_pools_[device].small_pool, &device_pools_[device].large_pool })
            {
                auto it = pool->allocated_blocks.find(ptr);
                if (it != pool->allocated_blocks.end())
                {
                    block = it->second;
                    pool_ptr = pool;
                    pool->allocated_blocks.erase(it);
                    break;
                }
            }

            if (!block)
            {
                // std::cerr << "Warning: deallocate called on unknown pointer" << std::endl;
                throw std::runtime_error("Anuj is a boy");
                return;
            }

            AllocationTracker::instance().on_free(ptr, device);

            block->allocated = false;
            block = pool_ptr->try_block_merge(block);

            BlockPool& final_pool = get_pool(block->size, device);

            final_pool.free_blocks.insert(block);
            final_pool.total_cached += block->size;
        }

        total_frees_++;
    }

    Block* CachingCUDAAllocator::cuda_alloc(size_t size, int device, cudaStream_t stream)
    {
        void* ptr = nullptr;
        cudaError_t err = cudaMallocAsync(&ptr, size, stream);
        if (err != cudaSuccess || !ptr) {
            fprintf(stderr, "[ALLOCATOR] cudaMallocAsync(size=%zu) failed: %s\n", size, cudaGetErrorString(err));
            return nullptr;
        }

        Block* block = new Block(ptr, size, device, stream);

        BlockPool& pool = get_pool(size, device);
        {
            std::lock_guard<std::mutex> lock(*device_pools_[device].mtx);
            pool.total_allocated += size;
            pool.peak_allocated = std::max(pool.peak_allocated, pool.total_allocated);
        }
        return block;
    }

    void CachingCUDAAllocator::cuda_free(Block* block)
    {
        cudaFreeAsync(block->ptr, block->stream);
        BlockPool& pool = get_pool(block->size, block->device_id);
        {
            std::lock_guard<std::mutex> lock(*device_pools_[block->device_id].mtx);
            pool.total_allocated -= block->size;
        }

        delete block;
    }

    void CachingCUDAAllocator::cuda_free_locked(Block* block)
    {
        // ASSUMES LOCK IS HELD
        if (block->prev == nullptr && block->next == nullptr) {
            cudaFreeAsync(block->ptr, block->stream);
            BlockPool& pool = get_pool(block->size, block->device_id);
            pool.total_allocated -= block->size;
        }
        delete block;
    }

    BlockPool& CachingCUDAAllocator::get_pool(size_t size, int device)
    {
        if (SizeClass::is_small(size)) {
            return device_pools_[device].small_pool;
        }
        else {
            return device_pools_[device].large_pool;
        }
    }

    void CachingCUDAAllocator::empty_cache()
    {
        // std::vector<Block*> blocks_to_free;
        // for (DevicePools& dev_pools : device_pools_)
        // {
        //      // Physical layout and pools are protected by device lock
        //      std::lock_guard<std::mutex> lock(*dev_pools.mtx);
        //      for (BlockPool* pool : { &dev_pools.small_pool, &dev_pools.large_pool })
        //      {
        //         for (Block* block : pool->free_blocks)
        //         {
        //             blocks_to_free.push_back(block);
        //         }
        //         pool->free_blocks.clear();
        //         pool->total_cached = 0;
        //      }
        // }

        // // You told me to add
        // // std::sort(blocks_to_free.begin(), blocks_to_free.end());
        // // blocks_to_free.erase(std::unique(blocks_to_free.begin(), blocks_to_free.end()), blocks_to_free.end());

        // for (Block* block : blocks_to_free)
        // {
        //     // we already cleared the pools' total_cached and free_blocks lists
        //     // but we need to update total_allocated and call Physical Free
        //     if (!block || !block->ptr) continue;
        //     cuda_free_locked(block);
        //     block->ptr = nullptr;
        // }
        
        // cudaDeviceSynchronize();

        std::set<Block*> unique_blocks_to_free;

        for (DevicePools& dev_pools : device_pools_) {
            std::lock_guard<std::mutex> lock(*dev_pools.mtx);
            for (BlockPool* pool : { &dev_pools.small_pool, &dev_pools.large_pool}) {
                for (Block* block : pool->free_blocks) {
                    unique_blocks_to_free.insert(block);
                }
                pool->free_blocks.clear();
                pool->total_cached = 0;
            }
        }

        for (Block* block : unique_blocks_to_free) {
            cuda_free_locked(block);
        }
        cudaDeviceSynchronize();
    }

    void CachingCUDAAllocator::ensure_stream_safety(Block* block, cudaStream_t target_stream)
    {
        if (block->stream == target_stream)
        {
            return;
        }

        if (block->stream == 0 || target_stream == 0)
        {
            cudaStreamSynchronize(block->stream);
        }
        else
        {
            cudaEvent_t event;
            cudaEventCreate(&event);
            cudaEventRecord(event, block->stream);
            cudaStreamWaitEvent(target_stream, event, 0);
            cudaEventDestroy(event);
        }

        block->stream = target_stream;
    }

    Block* CachingCUDAAllocator::try_split(Block* block, size_t size)
    {
        size_t remaining = block->size - size;

        if (remaining < SizeClass::kSmallSize)
        {
            return block; // too small to split }
        }
        
        num_splits_++;  // Track split operation
        
        void* new_ptr = static_cast<char*>(block->ptr) + size;
        Block* new_block = new Block(new_ptr, remaining, block->device_id, block->stream);
        new_block->is_split = true;

        block->size = size;
        block->is_split = true;

        new_block->prev = block;
        new_block->next = block->next;
        if (block->next)
        {
            block->next->prev = new_block;
        }
        block->next = new_block;


        BlockPool& pool = get_pool(remaining, block->device_id);
        // Already holding device lock from allocate()
        pool.free_blocks.insert(new_block);
        pool.total_cached += remaining;
        
        return block;
    }

    Block* BlockPool::find_free_block(size_t size, cudaStream_t /*stream*/)
    {
        Block search_key(nullptr, size, 0, nullptr);
        auto it = free_blocks.lower_bound(&search_key);

        if (it != free_blocks.end())
        {
            Block* block = *it;
            free_blocks.erase(it);
            return block;
        }
        return nullptr;
    }

    void BlockPool::return_block(Block* block)
    {
        free_blocks.insert(block);
        total_cached += block->size; 
    }

    Block* BlockPool::try_block_merge(Block* block)
    {
        // This is called from CachingCUDAAllocator with the device lock held.
        CachingCUDAAllocator& alloc = CachingCUDAAllocator::instance();

        auto remove_block = [&](Block* b) {
            bool removed = false;
            for (BlockPool* p : { &alloc.device_pools_[b->device_id].small_pool, 
                                 &alloc.device_pools_[b->device_id].large_pool }) {
                auto it = p->free_blocks.find(b);
                if (it != p->free_blocks.end()) {
                    p->free_blocks.erase(it);
                    p->total_cached -= b->size;
                    removed = true;
                    break;
                }
            }
            return removed;
        };

        if (block->prev && !block->prev->allocated)
        {
            Block* prev = block->prev;
            BlockPool& prev_pool = alloc.get_pool(prev->size, prev->device_id);
            
            prev_pool.free_blocks.erase(prev);
            prev_pool.total_cached -= prev->size;
            
            prev->size += block->size;
            prev->next = block->next;
            if (block->next)
            {
                block->next->prev = prev;
            }

            delete block;
            block = prev;
            alloc.num_merges_++;  // Track merge operation
        }

        if (block->next && !block->next->allocated)
        {
            Block* next = block->next;
            if (remove_block(next)) {
                block->size += next->size;
                block->next = next->next;
                if (next->next) {
                    next->next->prev = block;
                }
                delete next;
                alloc.num_merges_++;  // Track merge operation
            }

        }

        assert(block != nullptr);
        return block;
    }

    CachingCUDAAllocator::MemoryStats CachingCUDAAllocator::get_stats(int device) const
    {
        MemoryStats stats = {};

        auto add_pool_stats = [&](const BlockPool& pool, bool is_small) {
            size_t pool_active = pool.total_allocated - pool.total_cached;
            
            stats.active_current += pool_active;
            stats.allocated_current += pool.total_allocated;
            stats.reserved_current += pool.total_cached;
            stats.allocated_peak = std::max(stats.allocated_peak, pool.peak_allocated);
            
            if (is_small) {
                stats.small_pool_allocated += pool.total_allocated;
                stats.small_pool_cached += pool.total_cached;
            } else {
                stats.large_pool_allocated += pool.total_allocated;
                stats.large_pool_cached += pool.total_cached;
            }
        };

        if (device < 0)
        {
            for (const DevicePools& dev_pools : device_pools_)
            {
                std::lock_guard<std::mutex> lock(*dev_pools.mtx);
                add_pool_stats(dev_pools.small_pool, true);
                add_pool_stats(dev_pools.large_pool, false);
            }
        }
        else
        {
            if (device < (int)device_pools_.size()) {
                const DevicePools& dev_pools = device_pools_[device];
                std::lock_guard<std::mutex> lock(*dev_pools.mtx);
                add_pool_stats(dev_pools.small_pool, true);
                add_pool_stats(dev_pools.large_pool, false);
            }
        }

        {
            std::lock_guard<std::mutex> lock(stats_mutex_);
            stats.num_allocs = total_allocs_;
            stats.num_frees = total_frees_;
            stats.num_cache_hits = cache_hits_;
            stats.num_cache_misses = cache_misses_;
            stats.num_splits = num_splits_;
            stats.num_merges = num_merges_;
            stats.num_ooms = num_ooms_;
            stats.num_alloc_retries = num_alloc_retries_;
            stats.active_peak = peak_active_;
            stats.reserved_peak = peak_reserved_;
        }
        
        // Legacy compatibility
        stats.allocated = stats.active_current;
        stats.cached = stats.reserved_current;
        stats.peak = stats.allocated_peak;
        
        return stats;
    }

    void CachingCUDAAllocator::print_memory_summary() const
    {
        MemoryStats stats = get_stats();
        
        auto mb = [](size_t bytes) { return bytes / 1024.0 / 1024.0; };

        std::cerr << "\n==================== CUDA Caching Allocator Stats ====================\n";
        
        std::cerr << "\n--- Memory Usage ---\n";
        std::cerr << "  Active (in use):     " << mb(stats.active_current) << " MB (peak: " << mb(stats.active_peak) << " MB)\n";
        std::cerr << "  Allocated (CUDA):    " << mb(stats.allocated_current) << " MB (peak: " << mb(stats.allocated_peak) << " MB)\n";
        std::cerr << "  Reserved (cached):   " << mb(stats.reserved_current) << " MB (peak: " << mb(stats.reserved_peak) << " MB)\n";
        
        std::cerr << "\n--- Pool Breakdown ---\n";
        std::cerr << "  Small pool:          " << mb(stats.small_pool_allocated) << " MB allocated, " 
                  << mb(stats.small_pool_cached) << " MB cached\n";
        std::cerr << "  Large pool:          " << mb(stats.large_pool_allocated) << " MB allocated, " 
                  << mb(stats.large_pool_cached) << " MB cached\n";
        
        std::cerr << "\n--- Allocation Stats ---\n";
        std::cerr << "  Total allocations:   " << stats.num_allocs << "\n";
        std::cerr << "  Total frees:         " << stats.num_frees << "\n";
        std::cerr << "  Cache hits:          " << stats.num_cache_hits << " (" << stats.cache_hit_rate() << "%)\n";
        std::cerr << "  Cache misses:        " << stats.num_cache_misses << "\n";
        
        std::cerr << "\n--- Block Operations ---\n";
        std::cerr << "  Block splits:        " << stats.num_splits << "\n";
        std::cerr << "  Block merges:        " << stats.num_merges << "\n";
        
        std::cerr << "\n--- OOM Recovery ---\n";
        std::cerr << "  OOM events:          " << stats.num_ooms << "\n";
        std::cerr << "  Successful retries:  " << stats.num_alloc_retries << "\n";
        
        std::cerr << "\n--- Derived Metrics ---\n";
        std::cerr << "  Fragmentation:       " << stats.fragmentation_ratio() << "%\n";
        
        std::cerr << "=======================================================================\n\n";
    }

    void CachingCUDAAllocator::memset(void* ptr, int value, size_t bytes)
    {
        cudaStream_t stream = cuda::getCurrentStream();
        memsetAsync(ptr, value, bytes, stream);
    }

    void CachingCUDAAllocator::memcpy(void* dst, const void* src, size_t bytes, cudaMemcpyKind kind)
    {
        cudaStream_t stream = cuda::getCurrentStream();
        memcpyAsync(dst, src, bytes, kind, stream);
    }

    void CachingCUDAAllocator::memsetAsync(void* ptr, int value, size_t bytes, cudaStream_t stream)
    {
        cudaMemsetAsync(ptr, value, bytes, stream);
    }


    void CachingCUDAAllocator::memcpyAsync(void* dst, const void* src, size_t bytes, cudaMemcpyKind kind, cudaStream_t stream)
    {
        cudaMemcpyAsync(dst, src, bytes, kind, stream);
    }

}

===
#include "device/CachingCudaAllocator.h"
#include "device/SizeClass.h"
#include "device/AllocationTracker.h"
#include "device/DeviceCore.h"
#include <iostream>
#include <algorithm>

namespace OwnTensor
{
    CachingCUDAAllocator& CachingCUDAAllocator::instance()
    {
        static CachingCUDAAllocator allocator;
        return allocator;
    }

    CachingCUDAAllocator::CachingCUDAAllocator()
    {
        int device_count = 0;
        cudaGetDeviceCount(&device_count);
        device_pools_.resize(device_count);
    }

    CachingCUDAAllocator::~CachingCUDAAllocator()
    {
        empty_cache();
    }

    void* CachingCUDAAllocator::allocate(size_t bytes)
    {
        cudaStream_t stream = cuda::getCurrentStream();
        return allocate(bytes, stream);
    }

    void CachingCUDAAllocator::trim_to(size_t target_bytes)
    {
        for (DevicePools& pools : device_pools_)
        {
            std::lock_guard<std::mutex> lock(*pools.mtx);
            trim_pool(pools.small_pool, target_bytes / 2);
            trim_pool(pools.large_pool, target_bytes / 2);
        }
    }

    void CachingCUDAAllocator::trim_pool(BlockPool& pool, size_t target_bytes) {
    // ASSUMES LOCK IS HELD

    // Use a while loop with a safe forward-iterator check
    while (pool.total_cached > target_bytes && !pool.free_blocks.empty()) {
        
        // Always take the largest block to satisfy OOM quickly
        // In std::set, the largest element is at the end
        auto it = std::prev(pool.free_blocks.end());
        Block* block = *it;

        // CRITICAL: Update pointers and stats BEFORE the slow cudaFree
        pool.total_cached -= block->size;
        // pool.allocated_blocks.erase(block->ptr); // Not in allocated blocks if it's in free_blocks
        pool.free_blocks.erase(it);

        // LOCK #2: Stats update (must be very brief)
        {
            std::lock_guard<std::mutex> s_lock(stats_mutex_);
            total_frees_++;
        }

        // PHYSICAL FREE
        // We do this while still holding pool.mtx to prevent 
        // another thread from getting the same pointer from the driver
        cudaStreamSynchronize(block->stream);
        cudaFree(block->ptr); 
        
        assert(block != nullptr);
        delete block;
    }
}

    void* CachingCUDAAllocator::allocate(size_t bytes, cudaStream_t stream)
    {
        if (bytes == 0) return nullptr;

        int device;
        cudaGetDevice(&device);

        size_t alloc_size = SizeClass::round_size(bytes);

        BlockPool& preferred_pool = get_pool(alloc_size, device);
        BlockPool* actual_pool_ptr = &preferred_pool;

        Block* block = nullptr;
        {
            std::lock_guard<std::mutex> lock(*device_pools_[device].mtx);
            
            // Try preferred pool first
            block = preferred_pool.find_free_block(alloc_size, stream);
            
            // If missed in small pool, try large pool as fallback
            if (!block && SizeClass::is_small(alloc_size)) {
                block = device_pools_[device].large_pool.find_free_block(alloc_size, stream);
                if (block) {
                    // Pull from large pool!
                    actual_pool_ptr = &device_pools_[device].large_pool;
                }
            }

            if (block)
            {
                {
                    std::lock_guard<std::mutex> s_lock(stats_mutex_);
                    cache_hits_ += 1;
                }
                actual_pool_ptr->total_cached -= block->size;

                if (block->size >= alloc_size + SizeClass::kSmallSize)
                {
                    block = try_split(block, alloc_size);
                }
            }
        }

        if (block) {
            ensure_stream_safety(block, stream);
        }

        // Cache miss - need fresh CUDA allocation
        if (!block) {
            cache_misses_++;
            block = cuda_alloc(alloc_size, device, stream);
            if (!block) {
                // Log to system stderr directly to bypass potential stream hangs
                fprintf(stderr, "[ALLOCATOR] Hard OOM. Trimming cache...\n");
                num_ooms_++;
                
                this->trim_to(0); 
                
                block = cuda_alloc(alloc_size, device, stream);
                if (!block) {
                    fprintf(stderr, "[ALLOCATOR] FATAL: Still OOM after trim.\n");
                    throw std::runtime_error("CUDA OOM");
                }
                num_alloc_retries_++;
            }
        }


        block->allocated = true;
        block->req_size = bytes;
        block->stream = stream;

        {
            std::lock_guard<std::mutex> lock(*device_pools_[device].mtx);
            actual_pool_ptr->allocated_blocks[block->ptr] = block;
        }

        AllocationTracker::instance().on_alloc(block->ptr, bytes, device);

        total_allocs_++;
        return block->ptr;
    }

    void CachingCUDAAllocator::deallocate(void* ptr)
    {
        if (!ptr) return;

        Block* block = nullptr;
        BlockPool* pool_ptr = nullptr;
        int found_device = -1;

        // Search ALL device pools — the CUDA device context at deallocation time
        // may differ from allocation time (e.g., tensor allocated on device 1
        // but destructor runs while device 0 is the active context).
        for (int d = 0; d < (int)device_pools_.size(); ++d)
        {
            std::lock_guard<std::mutex> lock(*device_pools_[d].mtx);
            for (BlockPool* pool :
                { &device_pools_[d].small_pool, &device_pools_[d].large_pool })
            {
                auto it = pool->allocated_blocks.find(ptr);
                if (it != pool->allocated_blocks.end())
                {
                    block = it->second;
                    pool_ptr = pool;
                    found_device = d;
                    pool->allocated_blocks.erase(it);
                    break;
                }
            }
            if (block) break;
        }

        if (!block)
        {
            // Pointer genuinely not tracked — use cudaFreeAsync as last resort.
            cudaPointerAttributes attrs;
            cudaError_t err = cudaPointerGetAttributes(&attrs, ptr);
            if (err == cudaSuccess && (attrs.type == cudaMemoryTypeDevice || attrs.type == cudaMemoryTypeManaged)) {
                cudaFreeAsync(ptr, cuda::getCurrentStream());
            }
            return;
        }

        AllocationTracker::instance().on_free(ptr, found_device);

        {
            // Re-acquire the lock for the device we found
            std::lock_guard<std::mutex> lock(*device_pools_[found_device].mtx);
            block->allocated = false;
            block = pool_ptr->try_block_merge(block);

            BlockPool& final_pool = get_pool(block->size, found_device);
            final_pool.free_blocks.insert(block);
            final_pool.total_cached += block->size;
        }

        total_frees_++;
    }

    Block* CachingCUDAAllocator::cuda_alloc(size_t size, int device, cudaStream_t stream)
    {
        void* ptr = nullptr;
        cudaError_t err = cudaMallocAsync(&ptr, size, stream);
        if (err != cudaSuccess || !ptr) {
            fprintf(stderr, "[ALLOCATOR] cudaMallocAsync(size=%zu) failed: %s\n", size, cudaGetErrorString(err));
            return nullptr;
        }

        Block* block = new Block(ptr, size, device, stream);

        BlockPool& pool = get_pool(size, device);
        {
            std::lock_guard<std::mutex> lock(*device_pools_[device].mtx);
            pool.total_allocated += size;
            pool.peak_allocated = std::max(pool.peak_allocated, pool.total_allocated);
        }
        return block;
    }

    void CachingCUDAAllocator::cuda_free(Block* block)
    {
        cudaFreeAsync(block->ptr, block->stream);
        BlockPool& pool = get_pool(block->size, block->device_id);
        {
            std::lock_guard<std::mutex> lock(*device_pools_[block->device_id].mtx);
            pool.total_allocated -= block->size;
        }

        delete block;
    }

    void CachingCUDAAllocator::cuda_free_locked(Block* block)
    {
        // ASSUMES LOCK IS HELD
        if (block->prev == nullptr && block->next == nullptr) {
            cudaFreeAsync(block->ptr, block->stream);
            BlockPool& pool = get_pool(block->size, block->device_id);
            pool.total_allocated -= block->size;
        }
        delete block;
    }

    BlockPool& CachingCUDAAllocator::get_pool(size_t size, int device)
    {
        if (SizeClass::is_small(size)) {
            return device_pools_[device].small_pool;
        }
        else {
            return device_pools_[device].large_pool;
        }
    }

    void CachingCUDAAllocator::empty_cache()
    {
        // std::vector<Block*> blocks_to_free;
        // for (DevicePools& dev_pools : device_pools_)
        // {
        //      // Physical layout and pools are protected by device lock
        //      std::lock_guard<std::mutex> lock(*dev_pools.mtx);
        //      for (BlockPool* pool : { &dev_pools.small_pool, &dev_pools.large_pool })
        //      {
        //         for (Block* block : pool->free_blocks)
        //         {
        //             blocks_to_free.push_back(block);
        //         }
        //         pool->free_blocks.clear();
        //         pool->total_cached = 0;
        //      }
        // }

        // // You told me to add
        // // std::sort(blocks_to_free.begin(), blocks_to_free.end());
        // // blocks_to_free.erase(std::unique(blocks_to_free.begin(), blocks_to_free.end()), blocks_to_free.end());

        // for (Block* block : blocks_to_free)
        // {
        //     // we already cleared the pools' total_cached and free_blocks lists
        //     // but we need to update total_allocated and call Physical Free
        //     if (!block || !block->ptr) continue;
        //     cuda_free_locked(block);
        //     block->ptr = nullptr;
        // }
        
        // cudaDeviceSynchronize();

        std::set<Block*> unique_blocks_to_free;

        for (DevicePools& dev_pools : device_pools_) {
            std::lock_guard<std::mutex> lock(*dev_pools.mtx);
            for (BlockPool* pool : { &dev_pools.small_pool, &dev_pools.large_pool}) {
                for (Block* block : pool->free_blocks) {
                    unique_blocks_to_free.insert(block);
                }
                pool->free_blocks.clear();
                pool->total_cached = 0;
            }
        }

        for (Block* block : unique_blocks_to_free) {
            cuda_free_locked(block);
        }
        cudaDeviceSynchronize();
    }

    void CachingCUDAAllocator::ensure_stream_safety(Block* block, cudaStream_t target_stream)
    {
        if (block->stream == target_stream)
        {
            return;
        }

        if (block->stream == 0 || target_stream == 0)
        {
            cudaStreamSynchronize(block->stream);
        }
        else
        {
            cudaEvent_t event;
            cudaEventCreate(&event);
            cudaEventRecord(event, block->stream);
            cudaStreamWaitEvent(target_stream, event, 0);
            cudaEventDestroy(event);
        }

        block->stream = target_stream;
    }

    Block* CachingCUDAAllocator::try_split(Block* block, size_t size)
    {
        size_t remaining = block->size - size;

        if (remaining < SizeClass::kSmallSize)
        {
            return block; // too small to split }
        }
        
        num_splits_++;  // Track split operation
        
        void* new_ptr = static_cast<char*>(block->ptr) + size;
        Block* new_block = new Block(new_ptr, remaining, block->device_id, block->stream);
        new_block->is_split = true;

        block->size = size;
        block->is_split = true;

        new_block->prev = block;
        new_block->next = block->next;
        if (block->next)
        {
            block->next->prev = new_block;
        }
        block->next = new_block;


        BlockPool& pool = get_pool(remaining, block->device_id);
        // Already holding device lock from allocate()
        pool.free_blocks.insert(new_block);
        pool.total_cached += remaining;
        
        return block;
    }

    Block* BlockPool::find_free_block(size_t size, cudaStream_t /*stream*/)
    {
        Block search_key(nullptr, size, 0, nullptr);
        auto it = free_blocks.lower_bound(&search_key);

        if (it != free_blocks.end())
        {
            Block* block = *it;
            free_blocks.erase(it);
            return block;
        }
        return nullptr;
    }

    void BlockPool::return_block(Block* block)
    {
        free_blocks.insert(block);
        total_cached += block->size; 
    }

    Block* BlockPool::try_block_merge(Block* block)
    {
        // This is called from CachingCUDAAllocator with the device lock held.
        CachingCUDAAllocator& alloc = CachingCUDAAllocator::instance();

        auto remove_block = [&](Block* b) {
            bool removed = false;
            for (BlockPool* p : { &alloc.device_pools_[b->device_id].small_pool, 
                                 &alloc.device_pools_[b->device_id].large_pool }) {
                auto it = p->free_blocks.find(b);
                if (it != p->free_blocks.end()) {
                    p->free_blocks.erase(it);
                    p->total_cached -= b->size;
                    removed = true;
                    break;
                }
            }
            return removed;
        };

        if (block->prev && !block->prev->allocated)
        {
            Block* prev = block->prev;
            BlockPool& prev_pool = alloc.get_pool(prev->size, prev->device_id);
            
            prev_pool.free_blocks.erase(prev);
            prev_pool.total_cached -= prev->size;
            
            prev->size += block->size;
            prev->next = block->next;
            if (block->next)
            {
                block->next->prev = prev;
            }

            delete block;
            block = prev;
            alloc.num_merges_++;  // Track merge operation
        }

        if (block->next && !block->next->allocated)
        {
            Block* next = block->next;
            if (remove_block(next)) {
                block->size += next->size;
                block->next = next->next;
                if (next->next) {
                    next->next->prev = block;
                }
                delete next;
                alloc.num_merges_++;  // Track merge operation
            }

        }

        assert(block != nullptr);
        return block;
    }

    CachingCUDAAllocator::MemoryStats CachingCUDAAllocator::get_stats(int device) const
    {
        MemoryStats stats = {};

        auto add_pool_stats = [&](const BlockPool& pool, bool is_small) {
            size_t pool_active = pool.total_allocated - pool.total_cached;
            
            stats.active_current += pool_active;
            stats.allocated_current += pool.total_allocated;
            stats.reserved_current += pool.total_cached;
            stats.allocated_peak = std::max(stats.allocated_peak, pool.peak_allocated);
            
            if (is_small) {
                stats.small_pool_allocated += pool.total_allocated;
                stats.small_pool_cached += pool.total_cached;
            } else {
                stats.large_pool_allocated += pool.total_allocated;
                stats.large_pool_cached += pool.total_cached;
            }
        };

        if (device < 0)
        {
            for (const DevicePools& dev_pools : device_pools_)
            {
                std::lock_guard<std::mutex> lock(*dev_pools.mtx);
                add_pool_stats(dev_pools.small_pool, true);
                add_pool_stats(dev_pools.large_pool, false);
            }
        }
        else
        {
            if (device < (int)device_pools_.size()) {
                const DevicePools& dev_pools = device_pools_[device];
                std::lock_guard<std::mutex> lock(*dev_pools.mtx);
                add_pool_stats(dev_pools.small_pool, true);
                add_pool_stats(dev_pools.large_pool, false);
            }
        }

        {
            std::lock_guard<std::mutex> lock(stats_mutex_);
            stats.num_allocs = total_allocs_;
            stats.num_frees = total_frees_;
            stats.num_cache_hits = cache_hits_;
            stats.num_cache_misses = cache_misses_;
            stats.num_splits = num_splits_;
            stats.num_merges = num_merges_;
            stats.num_ooms = num_ooms_;
            stats.num_alloc_retries = num_alloc_retries_;
            stats.active_peak = peak_active_;
            stats.reserved_peak = peak_reserved_;
        }
        
        // Legacy compatibility
        stats.allocated = stats.active_current;
        stats.cached = stats.reserved_current;
        stats.peak = stats.allocated_peak;
        
        return stats;
    }

    void CachingCUDAAllocator::print_memory_summary() const
    {
        MemoryStats stats = get_stats();
        
        auto mb = [](size_t bytes) { return bytes / 1024.0 / 1024.0; };

        std::cerr << "\n==================== CUDA Caching Allocator Stats ====================\n";
        
        std::cerr << "\n--- Memory Usage ---\n";
        std::cerr << "  Active (in use):     " << mb(stats.active_current) << " MB (peak: " << mb(stats.active_peak) << " MB)\n";
        std::cerr << "  Allocated (CUDA):    " << mb(stats.allocated_current) << " MB (peak: " << mb(stats.allocated_peak) << " MB)\n";
        std::cerr << "  Reserved (cached):   " << mb(stats.reserved_current) << " MB (peak: " << mb(stats.reserved_peak) << " MB)\n";
        
        std::cerr << "\n--- Pool Breakdown ---\n";
        std::cerr << "  Small pool:          " << mb(stats.small_pool_allocated) << " MB allocated, " 
                  << mb(stats.small_pool_cached) << " MB cached\n";
        std::cerr << "  Large pool:          " << mb(stats.large_pool_allocated) << " MB allocated, " 
                  << mb(stats.large_pool_cached) << " MB cached\n";
        
        std::cerr << "\n--- Allocation Stats ---\n";
        std::cerr << "  Total allocations:   " << stats.num_allocs << "\n";
        std::cerr << "  Total frees:         " << stats.num_frees << "\n";
        std::cerr << "  Cache hits:          " << stats.num_cache_hits << " (" << stats.cache_hit_rate() << "%)\n";
        std::cerr << "  Cache misses:        " << stats.num_cache_misses << "\n";
        
        std::cerr << "\n--- Block Operations ---\n";
        std::cerr << "  Block splits:        " << stats.num_splits << "\n";
        std::cerr << "  Block merges:        " << stats.num_merges << "\n";
        
        std::cerr << "\n--- OOM Recovery ---\n";
        std::cerr << "  OOM events:          " << stats.num_ooms << "\n";
        std::cerr << "  Successful retries:  " << stats.num_alloc_retries << "\n";
        
        std::cerr << "\n--- Derived Metrics ---\n";
        std::cerr << "  Fragmentation:       " << stats.fragmentation_ratio() << "%\n";
        
        std::cerr << "=======================================================================\n\n";
    }

    void CachingCUDAAllocator::memset(void* ptr, int value, size_t bytes)
    {
        cudaStream_t stream = cuda::getCurrentStream();
        memsetAsync(ptr, value, bytes, stream);
    }

    void CachingCUDAAllocator::memcpy(void* dst, const void* src, size_t bytes, cudaMemcpyKind kind)
    {
        cudaStream_t stream = cuda::getCurrentStream();
        memcpyAsync(dst, src, bytes, kind, stream);
    }

    void CachingCUDAAllocator::memsetAsync(void* ptr, int value, size_t bytes, cudaStream_t stream)
    {
        cudaMemsetAsync(ptr, value, bytes, stream);
    }


    void CachingCUDAAllocator::memcpyAsync(void* dst, const void* src, size_t bytes, cudaMemcpyKind kind, cudaStream_t stream)
    {
        cudaMemcpyAsync(dst, src, bytes, kind, stream);
    }

}

```

---

### 2. Forward pass hang on rank 1

A stale `invalid argument` CUDA error on rank 1 blocked kernel launches. [AsTypeTensor.cpp](file:///home/blu-bridge25/Study/Code/TensorParallelismBeta/DTensor/Tensor-Implementations/src/core/AsTypeTensor.cpp) didn't call `cudaSetDevice` before CUDA ops.

**Fix**: Added `cudaSetDevice(this->device().index)` + `cudaGetLastError()` in [AsTypeTensor.cpp](file:///home/blu-bridge25/Study/Code/TensorParallelismBeta/DTensor/Tensor-Implementations/src/core/AsTypeTensor.cpp). Also reset `g_current_stream` to default (0) in forward to prevent NCCL stream from serializing compute ops.

```diff:AsTypeTensor.cpp
#include "core/Tensor.h"
#include "core/TensorDispatch.h"
#include "device/DeviceTransfer.h"
#include "dtype/DtypeTraits.h"
#include "ops/helpers/ConversionKernels.cuh"
#include "device/DeviceCore.h"

namespace OwnTensor
{
    Tensor Tensor::as_type(Dtype new_dtype) const {
        // Edge Case: If types are the same, just return a clone
        if (new_dtype == this->dtype()) {
            return this->clone();
        }

        //  Validation: Prevent invalid complex/scalar conversions
        bool src_is_complex = is_complex(this->dtype());
        bool dst_is_complex = is_complex(new_dtype);
        if (src_is_complex != dst_is_complex) {
            throw std::runtime_error(
                "Cannot convert between complex and non-complex types. " +
                get_dtype_name(this->dtype()) + " -> " + get_dtype_name(new_dtype)
            );
        }

        // 1. Create the destination tensor on the SAME device
        Tensor new_tensor(this->shape(), TensorOptions{new_dtype, this->device()});

        // 2. Get element count
        const size_t n = this->numel();

        //  3. Handle CUDA tensors
        if (this->is_cuda()) {
            #ifdef WITH_CUDA
                // Launch GPU kernel
                cudaStream_t stream = OwnTensor::cuda::getCurrentStream();
                convert_type_cuda_generic(this->data(), this->dtype(), new_tensor.data(), new_dtype, n, stream);
            #else
                throw std::runtime_error("CUDA support not compiled");
            #endif
        } else {
            //  4. Handle CPU tensors (original logic)
            const auto* src_untyped_ptr = this->data();
            auto* dst_untyped_ptr = new_tensor.data();

            // Nested dispatch for type conversion
            dispatch_by_dtype(this->dtype(), [&](auto src_type_placeholder) {
                using SrcType = decltype(src_type_placeholder);
                const auto* src_data = reinterpret_cast<const SrcType*>(src_untyped_ptr);

                dispatch_by_dtype(new_dtype, [&](auto dst_type_placeholder) {
                    using DstType = decltype(dst_type_placeholder);
                    auto* dst_data = reinterpret_cast<DstType*>(dst_untyped_ptr);

                    // Compile-time check: both must be complex or both must be non-complex
                    constexpr bool src_is_complex = 
                        std::is_same_v<SrcType, complex32_t> ||
                        std::is_same_v<SrcType, complex64_t> ||
                        std::is_same_v<SrcType, complex128_t>;
                    
                    constexpr bool dst_is_complex = 
                        std::is_same_v<DstType, complex32_t> ||
                        std::is_same_v<DstType, complex64_t> ||
                        std::is_same_v<DstType, complex128_t>;

                    constexpr bool src_is_packed = std::is_same_v<SrcType, float4_e2m1_2x_t>;
                    constexpr bool dst_is_packed = std::is_same_v<DstType, float4_e2m1_2x_t>;

                    if constexpr (src_is_packed || dst_is_packed) {
                        if constexpr (std::is_same_v<SrcType, DstType>) {
                             for (size_t i = 0; i < n; ++i) dst_data[i] = src_data[i];
                        } else {
                             // Not supported
                             // We can't throw here at compile time, but we can at runtime.
                             // However, we must ensure this branch compiles.
                             // Since this lambda is instantiated for ALL types, we can't use static_cast here.
                             // We just throw exception at runtime if this path is taken.
                             // But we need to make sure the code is valid C++.
                             // throw is valid.
                             throw std::runtime_error("Conversion to/from packed FP4 type is not supported via as_type");
                        }
                    }
                    else if constexpr (src_is_complex == dst_is_complex) {
                        // Valid conversion: both complex or both non-complex
                        for (size_t i = 0; i < n; ++i) {
                            if constexpr (src_is_complex) {
                                // Manual complex-to-complex conversion by accessing components
                                dst_data[i] = DstType(
                                    static_cast<decltype(dst_data[i].real())>(src_data[i].real()),
                                    static_cast<decltype(dst_data[i].imag())>(src_data[i].imag())
                                );
                            } else {
                                // Non-complex types can use static_cast
                                dst_data[i] = static_cast<DstType>(src_data[i]);
                            }
                        }
                    }
                    // If compile-time check fails, this branch won't be instantiated
                });
            });
        }

        // 5. Return the newly created tensor
        return new_tensor;
    }
}
===
#include "core/Tensor.h"
#include "core/TensorDispatch.h"
#include "device/DeviceTransfer.h"
#include "dtype/DtypeTraits.h"
#include "ops/helpers/ConversionKernels.cuh"
#include "device/DeviceCore.h"

namespace OwnTensor
{
    Tensor Tensor::as_type(Dtype new_dtype) const {
        // Edge Case: If types are the same, just return a clone
        if (new_dtype == this->dtype()) {
            return this->clone();
        }

        //  Validation: Prevent invalid complex/scalar conversions
        bool src_is_complex = is_complex(this->dtype());
        bool dst_is_complex = is_complex(new_dtype);
        if (src_is_complex != dst_is_complex) {
            throw std::runtime_error(
                "Cannot convert between complex and non-complex types. " +
                get_dtype_name(this->dtype()) + " -> " + get_dtype_name(new_dtype)
            );
        }

        // 1. Create the destination tensor on the SAME device
        Tensor new_tensor(this->shape(), TensorOptions{new_dtype, this->device()});

        // 2. Get element count
        const size_t n = this->numel();

        //  3. Handle CUDA tensors
        if (this->is_cuda()) {
            #ifdef WITH_CUDA
                // Ensure correct CUDA device context — the active device may differ
                // from the tensor's device (e.g., during a multi-GPU forward pass,
                // cross-device tensor destruction can change the device context).
                cudaSetDevice(this->device().index);
                cudaGetLastError(); // Clear any accumulated errors from prior ops
                
                // Launch GPU kernel
                cudaStream_t stream = OwnTensor::cuda::getCurrentStream();
                convert_type_cuda_generic(this->data(), this->dtype(), new_tensor.data(), new_dtype, n, stream);
            #else
                throw std::runtime_error("CUDA support not compiled");
            #endif
        } else {
            //  4. Handle CPU tensors (original logic)
            const auto* src_untyped_ptr = this->data();
            auto* dst_untyped_ptr = new_tensor.data();

            // Nested dispatch for type conversion
            dispatch_by_dtype(this->dtype(), [&](auto src_type_placeholder) {
                using SrcType = decltype(src_type_placeholder);
                const auto* src_data = reinterpret_cast<const SrcType*>(src_untyped_ptr);

                dispatch_by_dtype(new_dtype, [&](auto dst_type_placeholder) {
                    using DstType = decltype(dst_type_placeholder);
                    auto* dst_data = reinterpret_cast<DstType*>(dst_untyped_ptr);

                    // Compile-time check: both must be complex or both must be non-complex
                    constexpr bool src_is_complex = 
                        std::is_same_v<SrcType, complex32_t> ||
                        std::is_same_v<SrcType, complex64_t> ||
                        std::is_same_v<SrcType, complex128_t>;
                    
                    constexpr bool dst_is_complex = 
                        std::is_same_v<DstType, complex32_t> ||
                        std::is_same_v<DstType, complex64_t> ||
                        std::is_same_v<DstType, complex128_t>;

                    constexpr bool src_is_packed = std::is_same_v<SrcType, float4_e2m1_2x_t>;
                    constexpr bool dst_is_packed = std::is_same_v<DstType, float4_e2m1_2x_t>;

                    if constexpr (src_is_packed || dst_is_packed) {
                        if constexpr (std::is_same_v<SrcType, DstType>) {
                             for (size_t i = 0; i < n; ++i) dst_data[i] = src_data[i];
                        } else {
                             // Not supported
                             // We can't throw here at compile time, but we can at runtime.
                             // However, we must ensure this branch compiles.
                             // Since this lambda is instantiated for ALL types, we can't use static_cast here.
                             // We just throw exception at runtime if this path is taken.
                             // But we need to make sure the code is valid C++.
                             // throw is valid.
                             throw std::runtime_error("Conversion to/from packed FP4 type is not supported via as_type");
                        }
                    }
                    else if constexpr (src_is_complex == dst_is_complex) {
                        // Valid conversion: both complex or both non-complex
                        for (size_t i = 0; i < n; ++i) {
                            if constexpr (src_is_complex) {
                                // Manual complex-to-complex conversion by accessing components
                                dst_data[i] = DstType(
                                    static_cast<decltype(dst_data[i].real())>(src_data[i].real()),
                                    static_cast<decltype(dst_data[i].imag())>(src_data[i].imag())
                                );
                            } else {
                                // Non-complex types can use static_cast
                                dst_data[i] = static_cast<DstType>(src_data[i]);
                            }
                        }
                    }
                    // If compile-time check fails, this branch won't be instantiated
                });
            });
        }

        // 5. Return the newly created tensor
        return new_tensor;
    }
}
```

---

### 3. `loss / grad_accum_steps` breaks autograd

[operator/](file:///home/blu-bridge25/Study/Code/TensorParallelismBeta/DTensor/Tensor-Implementations/src/ScalarOps/cpu/ScalarOpsDispatcher.cpp#187-191) with integer doesn't have an autograd wrapper → `grad_fn = NULL` after division → backward doesn't propagate.

**Fix**: Use `loss.backward(&grad_scale)` with pre-computed `grad_scale = 1.0f / grad_accum_steps`, matching working [gpt2_test_new.cpp](file:///home/blu-bridge25/Study/Code/TensorParallelismBeta/DTensor/gpt2_test_new.cpp). Accumulate `loss.detach()` for logging.

---

### 4. Training backward hang — CudaTimer deadlock

`CudaTimer::get_elapsed_seconds()` calls `cudaEventSynchronize`, which blocks the host and desynchronizes ranks. Rank 0 enters backward (with NCCL all-reduce) before rank 1 → NCCL deadlock.

**Fix**: Removed all CudaTimer calls from the micro-step loop. Post-loop timers (clip, optim, step) are safe since all ranks have completed backward by then.

```diff:gpt2_tp_mlp_test.cpp

/**
 * @file gpt2wotying.cpp
 * @brief GPT-2 training script WITHOUT weight tying
 * 
 * This script implements GPT-2 training using custom tensor library with autograd support.
 * Architecture: Token Embedding -> Position Embedding -> MLP x n_layers -> Linear -> Cross Entropy
 * 
 * Key difference from gpt2_test.cpp: 
 * - Separate weights for token embedding and output projection (no weight tying)
 * - Token embedding uses forward() with gradient tracking
 * - Output projection uses its own trainable weight matrix
 */

#include <cstdint>
#include <iostream>
#include <iomanip>
#include <chrono>
#include <cmath>
#include <vector>
#include <string>
#include <fstream>
#include <filesystem> 

// Tensor library includes
#include "TensorLib.h"
#include "autograd/AutogradOps.h"
#include "autograd/operations/LossOps.h"
#include "nn/optimizer/Optim.h"
#include "mlp/activation.h"
#include "autograd/operations/EmbeddingOps.h"
#include "nn/NN.h"
#include "nn/DistributedNN.h"


// Dataloader
#include "Data_Loader/dl_test.cpp"

using namespace OwnTensor;

// =============================================================================
// Configuration
// =============================================================================

int rank, world_size;

struct CudaTimer {
    cudaEvent_t start, stop;
    CudaTimer() {
        cudaEventCreate(&start);
        cudaEventCreate(&stop);
    }
    ~CudaTimer() {
        cudaEventDestroy(start);
        cudaEventDestroy(stop);
    }
    void start_timer() { cudaEventRecord(start); }
    float get_elapsed_ms() {
        cudaEventRecord(stop);
        cudaEventSynchronize(stop);
        float ms = 0;
        cudaEventElapsedTime(&ms, start, stop);
        return ms;
    }
    double get_elapsed_seconds() {
        return get_elapsed_ms() / 1000.0;
    }
};

struct GPTConfig {
    int64_t batch_size = 8;
    int64_t context_length = 1024;
    int64_t vocab_size = 50304;  // GPT-2 vocab size
    int64_t n_embd = 384;
    int64_t n_layers = 3;
};

// =============================================================================
// Embedding Layer with Autograd Support
// =============================================================================

class Embedding : public nn::Module {
public:
    Tensor weight;  // [vocab_size, n_embd]
    Embedding() = default;
    Embedding(int64_t vocab_size, int64_t embed_dim, DeviceIndex device, uint64_t seed = 1234)
        : vocab_size_(vocab_size), embed_dim_(embed_dim)
    {
        // Initialize weight with small normal distribution
        TensorOptions opts = TensorOptions().with_dtype(Dtype::Float32)
                                          .with_device(device)
                                          .with_req_grad(true);
        weight = Tensor::randn<float>(Shape{{vocab_size, embed_dim}}, opts, seed, 0.02f);
        
        register_parameter(weight);
    }
    
    // Forward: indices [B, T] -> embeddings [B, T, C]
    // Standard forward with gradient tracking through embedding
    Tensor forward(const Tensor& indices) override {
        // Use autograd-aware embedding function for proper gradient flow
        return autograd::embedding(weight, indices);
    }
    
private:
    int64_t vocab_size_;
    int64_t embed_dim_;
};

// =============================================================================
// MLP Block
// =============================================================================

// Helper: Initialize nn::Linear weights with GPT-2 style (std=0.02)
// Creates tensors directly on the target device
// void init_linear_gpt2(dnn::DColumnLinear& layer, DeviceIndex device, float std = 0.02f, uint64_t seed = 1234) {
//     TensorOptions opts = TensorOptions().with_dtype(Dtype::Float32)
//                                         .with_device(device)
//                                         .with_req_grad(true);
//     auto shape = layer.weight.shape();
//     layer.weight = Tensor::randn<float>(shape, opts, seed, std);
//     if (layer.bias.is_valid()) {
//         layer.bias = Tensor::zeros(layer.bias.shape(), opts);
//         // layer.bias = Tensor::randn<float>(layer.bias.shape(), opts, seed, std);
//     }
//     // std::cout << "WEIGHT: " << std::endl;
//     // layer.weight.display();
//     // std::cout << "BIAS: " << std::endl;
//     // layer.bias.display();
// }

class MLP : public dnn::DModule {
public:
    nn::LayerNorm ln;       // LayerNorm before MLP
    dnn::DColumnLinear fc_up;       // Linear(n_embd, 4*n_embd)
    dnn::DRowLinear fc_down;     // Linear(4*n_embd, n_embd)
    dnn::DGeLU gelu;
    
    MLP(GPTConfig config, DeviceMesh& mesh, std::shared_ptr<ProcessGroupNCCL>& pg,  DeviceIndex device, uint64_t seed = 1234)
        : ln(config.n_embd),
          fc_up(mesh, pg, config.batch_size, config.context_length, config.n_embd, 4 * config.n_embd,{}, false, seed),
          fc_down(mesh, pg, config.batch_size, config.context_length,  4 * config.n_embd, config.n_embd, {}, false, 0.02f * (1.0f / std::sqrt(2.0f * static_cast<float>(config.n_layers))), seed)

    {
        // GPT-2 style initialization - create tensors directly on target device
        // init_linear_gpt2(fc_up, device, 0.02f, seed);


        
        // Scaled init for residual projection: std *= (2 * n_layers) ** -0.5
        // float scale = 1.0f / std::sqrt(2.0f * static_cast<float>(config.n_layers));
        // init_linear_gpt2(fc_down, device, 0.02f * scale, seed + 1);
        
        // Move LayerNorm to device
        ln.to(device);
        
        register_module(ln);
        register_module(fc_up);
        register_module(fc_down);
    }

    using dnn::DModule::register_module;
    void register_module(nn::LayerNorm& m) {
        register_parameter(&m.weight);
        if(m.bias.is_valid()) register_parameter(&m.bias);
    }
    
    // Forward: x [B, T, C] -> [B, T, C]
    DTensor forward( DTensor& x) {
        // Pre-Norm: ln(x)
        DTensor h;
        h.mutable_tensor() = ln.forward(x.mutable_tensor());
        
        // Up projection + GELU + Down projection
        h = fc_up.forward(h);
        h = gelu.forward(h);
        h = fc_down.forward(h);
        
        // Residual connection: x + MLP(x)
        h.mutable_tensor() = autograd::add(x.mutable_tensor(), h.mutable_tensor());
        return h;
    }
    
private:
    int64_t n_embd_;
};

// =============================================================================
// GPT Model (WITHOUT Weight Tying)
// =============================================================================

class GPT : public dnn::DModule {
public:
    GPTConfig config;
    DeviceMesh &mesh;
    DTensor x;
    Embedding wte;  // Token embedding
    Embedding wpe;  // Position embedding
    dnn::DSequential mlps;
    nn::LayerNorm ln_f; // Final LayerNorm
    // Tensor& W_out;
    Tensor pos;
    Tensor logits;
    // DTensor Didx;  // Input token indices

    // Component timing (accumulated per step, reset after printing)
    double t_tok_emb = 0, t_pos_emb = 0, t_mlp = 0, t_ln_f = 0, t_lm_head = 0;
    CudaTimer timer_tok_emb, timer_pos_emb, timer_mlp, timer_ln_f, timer_lm_head;

    GPT(GPTConfig cfg, DeviceMesh& mesh, std::shared_ptr<ProcessGroupNCCL>& pg, DeviceIndex device, uint64_t seed = 1234)
        : config(cfg), 
          mesh(mesh),
          wte(cfg.vocab_size, cfg.n_embd, device, seed = 1234),
          wpe(cfg.context_length, cfg.n_embd, device),
          ln_f(cfg.n_embd)
    {
        ln_f.to(device);
        
        Layout Input_layout(mesh,{config.batch_size,config.context_length,config.n_embd});

        x = DTensor(mesh, pg, Input_layout, "x_combined");

        // Create MLP blocks and add to Sequential
        for (int i = 0; i < cfg.n_layers; ++i) {
            mlps.add(std::make_shared<MLP>(config, mesh, pg, device, 1234));
        }
        
        // Separate output projection weight (no weight tying)
        // Shape: [n_embd, vocab_size] to compute: hidden @ W_out = logits
        // TensorOptions opts = TensorOptions().with_dtype(Dtype::Float32)
        //                                   .with_device(device)
        //                                   .with_req_grad(true);
        // // Use same initialization as token embeddings (std=0.02)

        // Layout pos_layout (mesh,{1, config.context_length});
        // pos = DTensor(mesh, pg, pos_layout, "Pos DTensor");

        std::vector<float> pos_data(config.context_length);
        for (int64_t i = 0; i < config.context_length; ++i) {
                pos_data[i] = static_cast<float>(i);
        }
        // pos.setData(pos_data);

        // Initialize input indices DTensor
        // Layout idx_layout(mesh, {cfg.batch_size, cfg.context_length});
        // Didx = DTensor(mesh, pg, idx_layout, "InputIndices");



 

        register_module(wte);
        register_module(wpe);
        register_module(mlps);
        register_module(ln_f);

        // Initialize W_out
        // TensorOptions opts = TensorOptions().with_dtype(Dtype::Float32)
        //                                   .with_device(device)
        //                                   .with_req_grad(true);
        // W_out = Tensor::randn<float>(Shape{{config.n_embd, config.vocab_size}}, opts, seed + 500, 0.02f);
        // W_out = wte.weight.t();
        // register_parameter(&W_out);
    }

    using dnn::DModule::register_module;
    void register_module(nn::LayerNorm& m) {
        register_parameter(&m.weight);
        if(m.bias.is_valid()) register_parameter(&m.bias);
    }
    void register_module(Embedding& m) {
        register_parameter(&m.weight);
    }
    
    void reset_timing() {
        t_tok_emb = t_pos_emb = t_mlp = t_ln_f = t_lm_head = 0;
    }

    void print_timing(int rank) {
        if (rank == 0) {
            std::cout << "  [LAYER] tok_emb: " << std::fixed << std::setprecision(1) << (t_tok_emb * 1000.0) << "ms"
                      << " | pos_emb: " << (t_pos_emb * 1000.0) << "ms"
                      << " | mlps: " << (t_mlp * 1000.0) << "ms"
                      << " | ln_f: " << (t_ln_f * 1000.0) << "ms"
                      << " | lm_head: " << (t_lm_head * 1000.0) << "ms"
                      << std::endl;
        }
    }

    // Forward: indices [B, T] -> logits [B, T, vocab_size]
    Tensor forward(Tensor& idx) {
        auto shape = idx.shape().dims;
        int64_t B = shape[0];
        int64_t T = shape[1];
        
        // Create position indices [T]
        pos = Tensor(Shape{{1, T}}, TensorOptions().with_dtype(Dtype::Int64).with_device(idx.device()));
        {
            Tensor pos_cpu(Shape{{1, T}}, TensorOptions().with_dtype(Dtype::Int64));
            
            if (idx.device().is_cuda()) {
                pos = pos_cpu.to(idx.device());
            } else {
                pos = pos_cpu;
            }
        }
        
        // --- Token Embedding ---

        // --- Token Embedding ---

        timer_tok_emb.start_timer();
        Tensor tok_emb = wte.forward(idx);  // [B, T, C]

        t_tok_emb += timer_tok_emb.get_elapsed_seconds();

        // --- Position Embedding ---
        timer_pos_emb.start_timer();
        Tensor pos_emb = wpe.forward(pos);  // [1, T, C] - broadcasts

        t_pos_emb += timer_pos_emb.get_elapsed_seconds();
        
        // Properly construct x with mesh/pg/layout from embedding output
 
        // Add embeddings
        x.mutable_tensor() = autograd::add(tok_emb, pos_emb);
        
        // --- MLP Blocks ---
        timer_mlp.start_timer();
        x = mlps.forward(x);

        t_mlp += timer_mlp.get_elapsed_seconds();
        
        // --- Final LayerNorm ---
        timer_ln_f.start_timer();
        x.mutable_tensor() = ln_f.forward(x.mutable_tensor());

        t_ln_f += timer_ln_f.get_elapsed_seconds();

        // --- LM Head ---
        timer_lm_head.start_timer();
        logits = autograd::matmul(x.mutable_tensor(), wte.weight.t());

        t_lm_head += timer_lm_head.get_elapsed_seconds();
        
        // Final projection to vocab size [B, T, vocab_size]
        // Uses separate W_out instead of wte.weight.t()

        // Tensor logits = autograd::matmul(x, W_out);
        
        return logits;
    }
};



// =============================================================================
// Learning Rate Scheduler
// =============================================================================

float get_lr(int step, float max_lr, float min_lr, int warmup_steps, int max_steps) {
    if (step < warmup_steps) {
        return max_lr * static_cast<float>(step + 1) / static_cast<float>(warmup_steps);
    }
    if (step > max_steps) {
        return min_lr;
    }
    float decay_ratio = static_cast<float>(step - warmup_steps) / static_cast<float>(max_steps - warmup_steps);
    float coeff = 0.5f * (1.0f + std::cos(M_PI * decay_ratio));
    return min_lr + coeff * (max_lr - min_lr);
}

// =============================================================================
// Main Training Loop
// =============================================================================

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    if(rank == 0){
        std::cout << "=== GPT-2 Tensor Parallel Training Script ===" << std::endl;
    }
    try {
        
        // Configuration
        GPTConfig config;
        config.batch_size = 8;
        config.context_length = 1024;
        config.vocab_size = 50304;
        config.n_embd = 384;
        config.n_layers = 3;
        
        // Training hyperparameters
        const int B = 8;           // Batch size
        const int T = 1024;        // Sequence length
        const int global_batch = 65536;  // Global batch size
        const int grad_accum_steps = global_batch / (B * T);
        
        const float max_lr = 1e-4f;  
        const float min_lr = max_lr * 0.1f;
        
        
        
        std::cout << "Configuration:" << std::endl;
        std::cout << "  vocab_size: " << config.vocab_size << std::endl;
        std::cout << "  context_length: " << config.context_length << std::endl;
        std::cout << "  n_embd: " << config.n_embd << std::endl;
        std::cout << "  n_layers: " << config.n_layers << std::endl;
        std::cout << "  B=" << B << ", T=" << T << std::endl;
        std::cout << "  global_batch: " << global_batch << std::endl;
        std::cout << "  grad_accum_steps: " << grad_accum_steps << std::endl;
        // std::cout << "  Weight Tying: DISABLED" << std::endl;
        
        // Set device - GPU-0 for training
        // int gpu_device = 0;  // Use GPU-0
        // int rank = 0;        // Rank for dataloader (0 for single-GPU training)
        // int world_size = 2;
        
        
        DeviceIndex device(Device::CUDA, rank);
        cudaSetDevice(rank);
        
        std::vector<int> ranks_vec(world_size);
        for (int i = 0; i < world_size; i++) ranks_vec[i] = i;
        DeviceMesh mesh({world_size}, ranks_vec);
        auto pg = mesh.get_process_group(0);
        
        std::cout << "\nInitializing model on CUDA device "<< device.index << "..." << std::endl;
        
        // Create model
        GPT model(config, mesh, pg, device);
        
        // Print parameter count
        std::vector<DTensor*> params = model.parameters();
        int64_t num_params = 0;
        for(auto& p : params) num_params += p->mutable_tensor().numel();
        
                const int max_steps = num_params * 5 / global_batch;
                const int warmup_steps = max_steps / 10;
        
        if(rank == 0){
            std::cout << "Number of parameters: " << num_params << std::endl;
            std::cout << "Number of steps: " << max_steps << std::endl;
            std::cout << "Number of warmup_steps: " << warmup_steps << std::endl;
        }
        
        // std::cout << "(Note: More params than weight-tied version due to separate W_out)" << std::endl;
        
        // Get all parameters
        // auto params = model.parameters(); // Already got above
        
        // Create optimizer
        dnn::AdamW optimizer(max_lr, 0.9f, 0.95f, 1e-8f, 0.1f);
        
        // Create data loaders
        std::string data_root = "/home/blu-bridge25/Study/Code/TensorParallelismBeta/DTensor/Data_Loader/Data/";
        DataLoaderLite train_loader(B, T, 0, 1, "train", data_root, true, 100000000);
        DataLoaderLite val_loader(B, T, 0, 1, "val", data_root, true, 100000000);
        
        CudaTimer timer_step, timer_data, timer_fwd, timer_loss, timer_bwd, timer_clip, timer_optim;
        
        if(rank == 0){
            std::cout << "\nStarting training..." << std::endl;
        }
        
        // Create CSV log file
        // Enable dynamic log filename generation
        std::string log_filename;
        std::ofstream log_file;

        if (rank == 0) {
            int log_idx = 1;
            while (true) {
                log_filename = "TP_MLP_Training_log" + std::to_string(log_idx) + ".csv";
                std::ifstream check(log_filename);
                if (!check.good()) break;
                log_idx++;
            }
            
            std::cout << "Saving logs to: " << log_filename << std::endl;
            
            // Save configuration
            std::string config_filename = "TP_MLP_Training_log" + std::to_string(log_idx) + "_config.txt";
            std::ofstream config_file(config_filename);
            config_file << "Configuration:\n";
            config_file << "  Batch_size: " << B << "\n";
            config_file << "  context_length: " << config.context_length << "\n";
            config_file << "  n_embd: " << config.n_embd << "\n";
            config_file << "  vocab_size: " << config.vocab_size << "\n";
            config_file << "  n_layers: " << config.n_layers << "\n";
            config_file << "  global_batch: " << global_batch << "\n";
            config_file << "  grad_accum_steps: " << grad_accum_steps << "\n";
            config_file << "  Number of parameters: " << num_params << "\n";
            config_file << "  Max Learning Rate: " << max_lr << "\n";
            config_file << "  Min Learning Rate: " << min_lr << "\n";
            config_file << "  Number of steps: " << max_steps << "\n";
            config_file << "  Number of warmup_steps: " << warmup_steps << "\n";
            config_file.close();

            log_file.open(log_filename);
            log_file << "step,loss,val_loss,lr,grad_norm,dt_ms,tok_per_sec, timer_data, timer_fwd, timer_loss, timer_bwd, timer_clip, timer_optim, timer_tok_emb, timer_pos_emb, timer_mlp, timer_ln_f, timer_lm_head \n";
            log_file << std::fixed << std::setprecision(6);
        }
        
        float val_loss_accum_log = -1.0f;  // -1 indicates no validation this step
        

        for (int step = 0; step < max_steps; ++step) {
            timer_step.start_timer();
            
            // Validation every 100 steps
            if (step % 100 == 0 || step == max_steps - 1) {
                val_loader.reset();
                float val_loss_accum = 0.0f;
                int val_loss_steps = 20;
                
                for (int val_step = 0; val_step < val_loss_steps; ++val_step) {
                    Batch batch = val_loader.next_batch();
                    Tensor x = batch.input.to(device);
                    Tensor y = batch.target.to(device);
                    
                    Tensor logits = model.forward(x);
                    Tensor loss = autograd::sparse_cross_entropy_loss(logits, y);
                    
                    Tensor loss_cpu = loss.to_cpu();
                    val_loss_accum += loss_cpu.data<float>()[0] / static_cast<float>(val_loss_steps);
                }
                
                std::cout << "validation loss: " << std::fixed << std::setprecision(4) << val_loss_accum << std::endl;
                val_loss_accum_log = val_loss_accum;
            }
            
            // Training step with component timing
            double time_data = 0, time_forward = 0, time_loss = 0, time_backward = 0, time_allreduce = 0, time_clip = 0, time_optim = 0;

            optimizer.zero_grad();
            float loss_accum = 0.0f;
            
            // Optimized: Accumulate loss on GPU to avoid CPU syncs
            Tensor loss_accum_gpu = Tensor::zeros(Shape{{1}}, TensorOptions().with_device(device));
            
            for (int micro_step = 0; micro_step < grad_accum_steps; ++micro_step) {
                
                // --- Data Loading ---
                timer_data.start_timer();
                Batch batch = train_loader.next_batch();
                Tensor x = batch.input.to(device);
                Tensor y = batch.target.to(device);
        
                time_data += timer_data.get_elapsed_seconds();
                
                // --- Forward Pass ---
                timer_fwd.start_timer();
                Tensor logits = model.forward(x);
        
                time_forward += timer_fwd.get_elapsed_seconds();
                
                // --- Loss Computation ---
                timer_loss.start_timer();
                // Tensor loss = dnn::vocab_parallel_cross_entropy(logits, y);
                Tensor loss = autograd::sparse_cross_entropy_loss(logits,y);
                loss = loss / grad_accum_steps;
                loss_accum_gpu = loss_accum_gpu + loss;
        
                time_loss += timer_loss.get_elapsed_seconds();
                
                // --- Backward Pass ---
                timer_bwd.start_timer();
                Tensor grad_scale = Tensor::full(Shape{{1}}, TensorOptions().with_device(loss.device()), 1.0f);
                loss.backward(&grad_scale);
                time_backward += timer_bwd.get_elapsed_seconds();
            }
            
            // --- All-reduce gradients for replicated parameters ---
            // auto t_ar_start = std::chrono::high_resolution_clock::now();
            // model.all_reduce_gradients(pg.get());
    
            // auto t_ar_end = std::chrono::high_resolution_clock::now();
            // time_allreduce = std::chrono::duration<double>(t_ar_end - t_ar_start).count();

        //    std::cout<< "Token embedding: " << std::endl;
        //     model.wte.weight.grad_view().display() ;
        //     std::cout<< "Position embedding: " << std::endl;
        //     model.wpe.weight.grad_view().display();
        //     model.mlps[0].fc_up.weight.grad_view().display();
            
            // Transfer accumulated loss to CPU once per step
            Tensor loss_cpu = loss_accum_gpu.to_cpu();
            loss_accum = loss_cpu.data<float>()[0];
            
            // NaN detection - early exit if training goes unstable
            if (std::isnan(loss_accum) || std::isinf(loss_accum)) {
                std::cerr << "ERROR: NaN/Inf detected in loss at step " << step << std::endl;
                log_file.close();
                return 1;
            }
            
            // --- Gradient Clipping ---
            timer_clip.start_timer();
            float norm = dnn::clip_grad_norm_dtensor_nccl(params, 1.0f, pg);
    
            time_clip = timer_clip.get_elapsed_seconds();
            
            // Update learning rate
            float lr = get_lr(step, max_lr, min_lr, warmup_steps, max_steps);
            optimizer.set_lr(lr);
            
            // --- Optimizer Step ---
            timer_optim.start_timer();
            optimizer.step(params);
            time_optim = timer_optim.get_elapsed_seconds();
            
            double dt = timer_step.get_elapsed_seconds();
            
            // Compute throughput
            int64_t tokens_processed = static_cast<int64_t>(B) * T * grad_accum_steps;
            double tokens_per_sec = static_cast<double>(tokens_processed) / dt;
            
            // Print training info
        if(rank == 0){
            std::cout << "step " << std::setw(5) << step 
                        << " | loss: " << std::fixed << std::setprecision(6) << loss_accum 
                        << " | lr " << std::scientific << std::setprecision(4) << lr 
                        << " | norm: " << std::fixed << std::setprecision(4) << norm 
                        << " | dt: " << std::fixed << std::setprecision(2) << (dt * 1000.0) << "ms"
                        << " | tok/sec: " << std::fixed << std::setprecision(2) << tokens_per_sec 
                        << " |       Time Left: " << std::fixed << std::setprecision(2) << ((max_steps-step) * dt * 1000.0)/3600 << " hrs : "<<((max_steps-step) * dt * 1000.0)/60 <<" mins "
                        << std::endl;
            
            // Component timing breakdown (in ms)
            std::cout << "  [TIMING] data: " << std::fixed << std::setprecision(1) << (time_data * 1000.0) << "ms"
                      << " | fwd: " << (time_forward * 1000.0) << "ms"
                      << " | loss: " << (time_loss * 1000.0) << "ms"
                      << " | bwd: " << (time_backward * 1000.0) << "ms"
                      << " | clip: " << (time_clip * 1000.0) << "ms"
                      << " | optim: " << (time_optim * 1000.0) << "ms"
                    //   << " | gradients allreduce: " << std::fixed << std::setprecision(1) << (time_allreduce * 1000.0) << "ms"
                      << std::endl;
            
            // Layer-level timing breakdown
            model.print_timing(rank);
            
            // Log metrics to CSV
            log_file << step << "," 
            << loss_accum << ","
            << val_loss_accum_log << ","
            << lr << ","
            << norm << ","
            << (dt * 1000.0) << ","
            << tokens_per_sec << ","
            << (time_data * 1000.0) << ","
            << (time_forward * 1000.0) << ","
            << (time_loss * 1000.0) << ","
            << (time_backward * 1000.0) << ","
            << (time_clip * 1000.0) << ","
            << (time_optim * 1000.0) << ","
            << (model.t_tok_emb * 1000.0) << ","
            << (model.t_pos_emb * 1000.0) << ","
            << (model.t_mlp * 1000.0) << ","
            << (model.t_ln_f * 1000.0) << ","
            << (model.t_lm_head * 1000.0) << "\n";

            log_file.flush();
            model.reset_timing();
        }
        val_loss_accum_log = -1.0f;  // Reset for next iteration
    }
    if(rank == 0){
            log_file.close();
            std::cout << "\nTraining log saved to: " << log_filename << std::endl;
        
            std::cout << "\n=== Training Complete ===" << std::endl;
        }
        return 0;
        
    } catch (const std::exception& e) {
        std::cerr << "ERROR: " << e.what() << __LINE__ << std::endl;
        return 1;
    }
}

===

/**
 * @file gpt2wotying.cpp
 * @brief GPT-2 training script WITHOUT weight tying
 * 
 * This script implements GPT-2 training using custom tensor library with autograd support.
 * Architecture: Token Embedding -> Position Embedding -> MLP x n_layers -> Linear -> Cross Entropy
 * 
 * Key difference from gpt2_test.cpp: 
 * - Separate weights for token embedding and output projection (no weight tying)
 * - Token embedding uses forward() with gradient tracking
 * - Output projection uses its own trainable weight matrix
 */

#include <cstdint>
#include <iostream>
#include <iomanip>
#include <chrono>
#include <cmath>
#include <vector>
#include <string>
#include <fstream>
#include <filesystem> 

// Tensor library includes
#include "TensorLib.h"
#include "device/DeviceCore.h"
#include "autograd/AutogradOps.h"
#include "autograd/operations/LossOps.h"
#include "nn/optimizer/Optim.h"
#include "mlp/activation.h"
#include "autograd/operations/EmbeddingOps.h"
#include "nn/NN.h"
#include "nn/DistributedNN.h"


// Dataloader
#include "Data_Loader/dl_test.cpp"

using namespace OwnTensor;

// =============================================================================
// Configuration
// =============================================================================

int rank, world_size;

struct CudaTimer {
    cudaEvent_t start, stop;
    CudaTimer() {
        cudaEventCreate(&start);
        cudaEventCreate(&stop);
    }
    ~CudaTimer() {
        cudaEventDestroy(start);
        cudaEventDestroy(stop);
    }
    void start_timer() { cudaEventRecord(start); }
    float get_elapsed_ms() {
        cudaEventRecord(stop);
        cudaEventSynchronize(stop);
        float ms = 0;
        cudaEventElapsedTime(&ms, start, stop);
        return ms;
    }
    double get_elapsed_seconds() {
        return get_elapsed_ms() / 1000.0;
    }
};

struct GPTConfig {
    int64_t batch_size = 8;
    int64_t context_length = 1024;
    int64_t vocab_size = 50304;  // GPT-2 vocab size
    int64_t n_embd = 384;
    int64_t n_layers = 3;
};

// =============================================================================
// Embedding Layer with Autograd Support
// =============================================================================

class Embedding : public nn::Module {
public:
    Tensor weight;  // [vocab_size, n_embd]
    Embedding() = default;
    Embedding(int64_t vocab_size, int64_t embed_dim, DeviceIndex device, uint64_t seed = 1234)
        : vocab_size_(vocab_size), embed_dim_(embed_dim)
    {
        // Initialize weight with small normal distribution
        TensorOptions opts = TensorOptions().with_dtype(Dtype::Float32)
                                          .with_device(device)
                                          .with_req_grad(true);
        weight = Tensor::randn<float>(Shape{{vocab_size, embed_dim}}, opts, seed, 0.02f);
        
        register_parameter(weight);
    }
    
    // Forward: indices [B, T] -> embeddings [B, T, C]
    // Standard forward with gradient tracking through embedding
    Tensor forward(const Tensor& indices) override {
        // Use autograd-aware embedding function for proper gradient flow
        return autograd::embedding(weight, indices);
    }
    
private:
    int64_t vocab_size_;
    int64_t embed_dim_;
};

// =============================================================================
// MLP Block
// =============================================================================

// Helper: Initialize nn::Linear weights with GPT-2 style (std=0.02)
// Creates tensors directly on the target device
// void init_linear_gpt2(dnn::DColumnLinear& layer, DeviceIndex device, float std = 0.02f, uint64_t seed = 1234) {
//     TensorOptions opts = TensorOptions().with_dtype(Dtype::Float32)
//                                         .with_device(device)
//                                         .with_req_grad(true);
//     auto shape = layer.weight.shape();
//     layer.weight = Tensor::randn<float>(shape, opts, seed, std);
//     if (layer.bias.is_valid()) {
//         layer.bias = Tensor::zeros(layer.bias.shape(), opts);
//         // layer.bias = Tensor::randn<float>(layer.bias.shape(), opts, seed, std);
//     }
//     // std::cout << "WEIGHT: " << std::endl;
//     // layer.weight.display();
//     // std::cout << "BIAS: " << std::endl;
//     // layer.bias.display();
// }

class MLP : public dnn::DModule {
public:
    nn::LayerNorm ln;       // LayerNorm before MLP
    dnn::DColumnLinear fc_up;       // Linear(n_embd, 4*n_embd)
    dnn::DRowLinear fc_down;     // Linear(4*n_embd, n_embd)
    dnn::DGeLU gelu;
    
    MLP(GPTConfig config, DeviceMesh& mesh, std::shared_ptr<ProcessGroupNCCL>& pg,  DeviceIndex device, uint64_t seed = 1234)
        : ln(config.n_embd),
          fc_up(mesh, pg, config.batch_size, config.context_length, config.n_embd, 4 * config.n_embd,{}, false, seed),
          fc_down(mesh, pg, config.batch_size, config.context_length,  4 * config.n_embd, config.n_embd, {}, false, 0.02f * (1.0f / std::sqrt(2.0f * static_cast<float>(config.n_layers))), seed)

    {
        // GPT-2 style initialization - create tensors directly on target device
        // init_linear_gpt2(fc_up, device, 0.02f, seed);


        
        // Scaled init for residual projection: std *= (2 * n_layers) ** -0.5
        // float scale = 1.0f / std::sqrt(2.0f * static_cast<float>(config.n_layers));
        // init_linear_gpt2(fc_down, device, 0.02f * scale, seed + 1);
        
        // Move LayerNorm to device
        ln.to(device);
        
        register_module(ln);
        register_module(fc_up);
        register_module(fc_down);
    }

    using dnn::DModule::register_module;
    void register_module(nn::LayerNorm& m) {
        register_parameter(&m.weight);
        if(m.bias.is_valid()) register_parameter(&m.bias);
    }
    
    // Forward: x [B, T, C] -> [B, T, C]
    DTensor forward( DTensor& x) {
        // Pre-Norm: ln(x)
        DTensor h;
        h.mutable_tensor() = ln.forward(x.mutable_tensor());
        
        // Up projection + GELU + Down projection
        h = fc_up.forward(h);
        h = gelu.forward(h);
        h = fc_down.forward(h);
        
        // Residual connection: x + MLP(x)
        h.mutable_tensor() = autograd::add(x.mutable_tensor(), h.mutable_tensor());
        return h;
    }
    
private:
    int64_t n_embd_;
};

// =============================================================================
// GPT Model (WITHOUT Weight Tying)
// =============================================================================

class GPT : public dnn::DModule {
public:
    GPTConfig config;
    DeviceMesh &mesh;
    DTensor x;
    Embedding wte;  // Token embedding
    Embedding wpe;  // Position embedding
    dnn::DSequential mlps;
    nn::LayerNorm ln_f; // Final LayerNorm
    // Tensor& W_out;
    Tensor pos;
    Tensor logits;
    // DTensor Didx;  // Input token indices

    // Component timing (accumulated per step, reset after printing)
    double t_tok_emb = 0, t_pos_emb = 0, t_mlp = 0, t_ln_f = 0, t_lm_head = 0;
    CudaTimer timer_tok_emb, timer_pos_emb, timer_mlp, timer_ln_f, timer_lm_head;

    GPT(GPTConfig cfg, DeviceMesh& mesh, std::shared_ptr<ProcessGroupNCCL>& pg, DeviceIndex device, uint64_t seed = 1234)
        : config(cfg), 
          mesh(mesh),
          wte(cfg.vocab_size, cfg.n_embd, device, seed = 1234),
          wpe(cfg.context_length, cfg.n_embd, device),
          ln_f(cfg.n_embd)
    {
        ln_f.to(device);
        
        Layout Input_layout(mesh,{config.batch_size,config.context_length,config.n_embd});

        x = DTensor(mesh, pg, Input_layout, "x_combined");

        // Create MLP blocks and add to Sequential
        for (int i = 0; i < cfg.n_layers; ++i) {
            mlps.add(std::make_shared<MLP>(config, mesh, pg, device, 1234));
        }
        
        // Separate output projection weight (no weight tying)
        // Shape: [n_embd, vocab_size] to compute: hidden @ W_out = logits
        // TensorOptions opts = TensorOptions().with_dtype(Dtype::Float32)
        //                                   .with_device(device)
        //                                   .with_req_grad(true);
        // // Use same initialization as token embeddings (std=0.02)

        // Create position tensor ONCE with [0, 1, 2, ..., T-1] data
        {
            Tensor pos_cpu(Shape{{1, config.context_length}}, TensorOptions().with_dtype(Dtype::Int64));
            int64_t* pos_ptr = pos_cpu.data<int64_t>();
            for (int64_t i = 0; i < config.context_length; ++i) {
                pos_ptr[i] = i;
            }
            pos = pos_cpu.to(device);
        }



 

        register_module(wte);
        register_module(wpe);
        register_module(mlps);
        register_module(ln_f);

        // Initialize W_out
        // TensorOptions opts = TensorOptions().with_dtype(Dtype::Float32)
        //                                   .with_device(device)
        //                                   .with_req_grad(true);
        // W_out = Tensor::randn<float>(Shape{{config.n_embd, config.vocab_size}}, opts, seed + 500, 0.02f);
        // W_out = wte.weight.t();
        // register_parameter(&W_out);
    }

    using dnn::DModule::register_module;
    void register_module(nn::LayerNorm& m) {
        register_parameter(&m.weight);
        if(m.bias.is_valid()) register_parameter(&m.bias);
    }
    void register_module(Embedding& m) {
        register_parameter(&m.weight);
    }
    
    void reset_timing() {
        t_tok_emb = t_pos_emb = t_mlp = t_ln_f = t_lm_head = 0;
    }

    void print_timing(int rank) {
        if (rank == 0) {
            std::cout << "  [LAYER] tok_emb: " << std::fixed << std::setprecision(1) << (t_tok_emb * 1000.0) << "ms"
                      << " | pos_emb: " << (t_pos_emb * 1000.0) << "ms"
                      << " | mlps: " << (t_mlp * 1000.0) << "ms"
                      << " | ln_f: " << (t_ln_f * 1000.0) << "ms"
                      << " | lm_head: " << (t_lm_head * 1000.0) << "ms"
                      << std::endl;
        }
    }

    // Forward: indices [B, T] -> logits [B, T, vocab_size]
    Tensor forward(Tensor& idx) {
        // Reset to default stream for compute operations.
        // DTensor constructor sets g_current_stream to the NCCL stream,
        // which causes cudaMallocAsync to serialize behind NCCL operations
        // and deadlock when ranks are desynchronized.
        OwnTensor::cuda::setCurrentStream(0);
        
        // --- Token Embedding ---
        Tensor tok_emb = wte.forward(idx);  // [B, T, C]

        // --- Position Embedding (uses pre-created pos tensor) ---
        Tensor pos_emb = wpe.forward(pos);  // [1, T, C] - broadcasts

        // Add embeddings
        x.mutable_tensor() = autograd::add(tok_emb, pos_emb);

        // --- MLP Blocks ---
        x = mlps.forward(x);
        
        // --- Final LayerNorm ---
        x.mutable_tensor() = ln_f.forward(x.mutable_tensor());

        // --- LM Head ---
        logits = autograd::matmul(x.mutable_tensor(), wte.weight.t());
        
        return logits;
    }
};



// =============================================================================
// Learning Rate Scheduler
// =============================================================================

float get_lr(int step, float max_lr, float min_lr, int warmup_steps, int max_steps) {
    if (step < warmup_steps) {
        return max_lr * static_cast<float>(step + 1) / static_cast<float>(warmup_steps);
    }
    if (step > max_steps) {
        return min_lr;
    }
    float decay_ratio = static_cast<float>(step - warmup_steps) / static_cast<float>(max_steps - warmup_steps);
    float coeff = 0.5f * (1.0f + std::cos(M_PI * decay_ratio));
    return min_lr + coeff * (max_lr - min_lr);
}

// =============================================================================
// Main Training Loop
// =============================================================================

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    if(rank == 0){
        std::cout << "=== GPT-2 Tensor Parallel Training Script ===" << std::endl;
    }
    try {
        
        // Configuration
        GPTConfig config;
        config.batch_size = 8;
        config.context_length = 1024;
        config.vocab_size = 50304;
        config.n_embd = 384;
        config.n_layers = 3;
        
        // Training hyperparameters
        const int B = 8;           // Batch size
        const int T = 1024;        // Sequence length
        const int global_batch = 65536;  // Global batch size
        const int grad_accum_steps = global_batch / (B * T);
        
        const float max_lr = 1e-4f;  
        const float min_lr = max_lr * 0.1f;
        
        
        
        std::cout << "Configuration:" << std::endl;
        std::cout << "  vocab_size: " << config.vocab_size << std::endl;
        std::cout << "  context_length: " << config.context_length << std::endl;
        std::cout << "  n_embd: " << config.n_embd << std::endl;
        std::cout << "  n_layers: " << config.n_layers << std::endl;
        std::cout << "  B=" << B << ", T=" << T << std::endl;
        std::cout << "  global_batch: " << global_batch << std::endl;
        std::cout << "  grad_accum_steps: " << grad_accum_steps << std::endl;
        // std::cout << "  Weight Tying: DISABLED" << std::endl;
        
        // Set device - GPU-0 for training
        // int gpu_device = 0;  // Use GPU-0
        // int rank = 0;        // Rank for dataloader (0 for single-GPU training)
        // int world_size = 2;
        
        
        DeviceIndex device(Device::CUDA, rank);
        cudaSetDevice(rank);
        
        std::vector<int> ranks_vec(world_size);
        for (int i = 0; i < world_size; i++) ranks_vec[i] = i;
        DeviceMesh mesh({world_size}, ranks_vec);
        auto pg = mesh.get_process_group(0);
        
        std::cout << "\nInitializing model on CUDA device "<< device.index << "..." << std::endl;
        
        // Create model
        GPT model(config, mesh, pg, device);
        
        // Print parameter count
        std::vector<DTensor*> params = model.parameters();
        int64_t num_params = 0;
        for(auto& p : params) num_params += p->mutable_tensor().numel();
        
                const int max_steps = num_params * 5 / global_batch;
                const int warmup_steps = max_steps / 10;
        
        if(rank == 0){
            std::cout << "Number of parameters: " << num_params << std::endl;
            std::cout << "Number of steps: " << max_steps << std::endl;
            std::cout << "Number of warmup_steps: " << warmup_steps << std::endl;
        }
        
        // std::cout << "(Note: More params than weight-tied version due to separate W_out)" << std::endl;
        
        // Get all parameters
        // auto params = model.parameters(); // Already got above
        
        // Create optimizer
        dnn::AdamW optimizer(max_lr, 0.9f, 0.95f, 1e-8f, 0.1f);
        
        // Create data loaders
        std::string data_root = "/home/blu-bridge25/Study/Code/TensorParallelismBeta/DTensor/Data_Loader/Data/";
        DataLoaderLite train_loader(B, T, rank, 1, "train", data_root, true, 100000000);
        DataLoaderLite val_loader(B, T, rank, 1, "val", data_root, true, 100000000);
        
        CudaTimer timer_step, timer_data, timer_fwd, timer_loss, timer_bwd, timer_clip, timer_optim;
        
        if(rank == 0){
            std::cout << "\nStarting training..." << std::endl;
        }
        
        // Create CSV log file
        // Enable dynamic log filename generation
        std::string log_filename;
        std::ofstream log_file;

        if (rank == 0) {
            int log_idx = 1;
            while (true) {
                log_filename = "TP_MLP_Training_log" + std::to_string(log_idx) + ".csv";
                std::ifstream check(log_filename);
                if (!check.good()) break;
                log_idx++;
            }
            
            std::cout << "Saving logs to: " << log_filename << std::endl;
            
            // Save configuration
            std::string config_filename = "TP_MLP_Training_log" + std::to_string(log_idx) + "_config.txt";
            std::ofstream config_file(config_filename);
            config_file << "Configuration:\n";
            config_file << "  Batch_size: " << B << "\n";
            config_file << "  context_length: " << config.context_length << "\n";
            config_file << "  n_embd: " << config.n_embd << "\n";
            config_file << "  vocab_size: " << config.vocab_size << "\n";
            config_file << "  n_layers: " << config.n_layers << "\n";
            config_file << "  global_batch: " << global_batch << "\n";
            config_file << "  grad_accum_steps: " << grad_accum_steps << "\n";
            config_file << "  Number of parameters: " << num_params << "\n";
            config_file << "  Max Learning Rate: " << max_lr << "\n";
            config_file << "  Min Learning Rate: " << min_lr << "\n";
            config_file << "  Number of steps: " << max_steps << "\n";
            config_file << "  Number of warmup_steps: " << warmup_steps << "\n";
            config_file.close();

            log_file.open(log_filename);
            log_file << "step,loss,val_loss,lr,grad_norm,dt_ms,tok_per_sec, timer_data, timer_fwd, timer_loss, timer_bwd, timer_clip, timer_optim, timer_tok_emb, timer_pos_emb, timer_mlp, timer_ln_f, timer_lm_head \n";
            log_file << std::fixed << std::setprecision(6);
        }
        
        float val_loss_accum_log = -1.0f;  // -1 indicates no validation this step
        

        for (int step = 0; step < max_steps; ++step) {
            timer_step.start_timer();
            
            // Validation every 100 steps
            if (step % 100 == 0 || step == max_steps - 1) {
                val_loader.reset();
                float val_loss_accum = 0.0f;
                int val_loss_steps = 20;
                
                for (int val_step = 0; val_step < val_loss_steps; ++val_step) {
                    Batch batch = val_loader.next_batch();
                    Tensor x = batch.input.to(device);
                    Tensor y = batch.target.to(device);
                    
                    Tensor logits = model.forward(x);
                    Tensor loss = autograd::sparse_cross_entropy_loss(logits, y);
                    
                    Tensor loss_cpu = loss.to_cpu();
                    val_loss_accum += loss_cpu.data<float>()[0] / static_cast<float>(val_loss_steps);
                }
                
                std::cout << "validation loss: " << std::fixed << std::setprecision(4) << val_loss_accum << std::endl;
                val_loss_accum_log = val_loss_accum;
            }
            
            // Training step with component timing
            double time_data = 0, time_forward = 0, time_loss = 0, time_backward = 0, time_allreduce = 0, time_clip = 0, time_optim = 0;

            optimizer.zero_grad();
            float loss_accum = 0.0f;
            
            // Optimized: Accumulate loss on GPU to avoid CPU syncs
            Tensor loss_accum_gpu = Tensor::zeros(Shape{{1}}, TensorOptions().with_device(device));

            // Cache grad_scale outside the loop — same value every micro-step
            static Tensor grad_scale = Tensor::full(Shape{{1}}, TensorOptions().with_device(device),
                                                     1.0f / static_cast<float>(grad_accum_steps));
            
            for (int micro_step = 0; micro_step < grad_accum_steps; ++micro_step) {
                
                // --- Data Loading ---
                Batch batch = train_loader.next_batch();
                Tensor x = batch.input.to(device);
                Tensor y = batch.target.to(device);
                
                // --- Forward Pass ---
                Tensor logits = model.forward(x);

                // [DEBUG] Autograd graph diagnostics (first step only)
                if (step == 0 && micro_step == 0 && rank == 0) {
                    std::cout << "[DIAG] logits.requires_grad() = " << logits.requires_grad() << std::endl;
                    std::cout << "[DIAG] logits.grad_fn() = " << (logits.grad_fn() ? logits.grad_fn()->name() : "NULL") << std::endl;
                }
                
                // --- Loss Computation ---
                Tensor loss = autograd::sparse_cross_entropy_loss(logits, y);

                if (step == 0 && micro_step == 0 && rank == 0) {
                    std::cout << "[DIAG] loss requires_grad = " << loss.requires_grad() << std::endl;
                    std::cout << "[DIAG] loss grad_fn = " << (loss.grad_fn() ? loss.grad_fn()->name() : "NULL") << std::endl;
                }

                // Accumulate detached loss on GPU (no autograd graph, no CPU sync)
                loss_accum_gpu = loss_accum_gpu + loss.detach();
                
                // --- Backward Pass (with gradient scaling for accumulation) ---
                loss.backward(&grad_scale);
            }
            
            // --- All-reduce gradients for replicated parameters ---
            // auto t_ar_start = std::chrono::high_resolution_clock::now();
            // model.all_reduce_gradients(pg.get());
    
            // auto t_ar_end = std::chrono::high_resolution_clock::now();
            // time_allreduce = std::chrono::duration<double>(t_ar_end - t_ar_start).count();

        //    std::cout<< "Token embedding: " << std::endl;
        //     model.wte.weight.grad_view().display() ;
        //     std::cout<< "Position embedding: " << std::endl;
        //     model.wpe.weight.grad_view().display();
        //     model.mlps[0].fc_up.weight.grad_view().display();
            
            // Transfer accumulated loss to CPU once per step
            Tensor loss_cpu = loss_accum_gpu.to_cpu();
            loss_accum = loss_cpu.data<float>()[0] / static_cast<float>(grad_accum_steps);
            
            // NaN detection - early exit if training goes unstable
            if (std::isnan(loss_accum) || std::isinf(loss_accum)) {
                std::cerr << "ERROR: NaN/Inf detected in loss at step " << step << std::endl;
                log_file.close();
                return 1;
            }
            
            // --- Gradient Clipping ---
            timer_clip.start_timer();
            float norm = dnn::clip_grad_norm_dtensor_nccl(params, 1.0f, pg);
    
            time_clip = timer_clip.get_elapsed_seconds();
            
            // Update learning rate
            float lr = get_lr(step, max_lr, min_lr, warmup_steps, max_steps);
            optimizer.set_lr(lr);
            
            // --- Optimizer Step ---
            timer_optim.start_timer();
            optimizer.step(params);
            time_optim = timer_optim.get_elapsed_seconds();
            
            double dt = timer_step.get_elapsed_seconds();
            
            // Compute throughput
            int64_t tokens_processed = static_cast<int64_t>(B) * T * grad_accum_steps;
            double tokens_per_sec = static_cast<double>(tokens_processed) / dt;
            
            // Print training info
        if(rank == 0){
            std::cout << "step " << std::setw(5) << step 
                        << " | loss: " << std::fixed << std::setprecision(6) << loss_accum 
                        << " | lr " << std::scientific << std::setprecision(4) << lr 
                        << " | norm: " << std::fixed << std::setprecision(4) << norm 
                        << " | dt: " << std::fixed << std::setprecision(2) << (dt * 1000.0) << "ms"
                        << " | tok/sec: " << std::fixed << std::setprecision(2) << tokens_per_sec 
                        << " |       Time Left: " << std::fixed << std::setprecision(2) << ((max_steps-step) * dt * 1000000.0)/3600 << " hrs : "<<((max_steps-step) * dt * 1000000.0)/60 <<" mins "
                        << std::endl;
            
            // Component timing breakdown (in ms)
            std::cout << "  [TIMING] data: " << std::fixed << std::setprecision(1) << (time_data * 1000.0) << "ms"
                      << " | fwd: " << (time_forward * 1000.0) << "ms"
                      << " | loss: " << (time_loss * 1000.0) << "ms"
                      << " | bwd: " << (time_backward * 1000.0) << "ms"
                      << " | clip: " << (time_clip * 1000.0) << "ms"
                      << " | optim: " << (time_optim * 1000.0) << "ms"
                    //   << " | gradients allreduce: " << std::fixed << std::setprecision(1) << (time_allreduce * 1000.0) << "ms"
                      << std::endl;
            
            // Layer-level timing breakdown
            model.print_timing(rank);
            
            // Log metrics to CSV
            log_file << step << "," 
            << loss_accum << ","
            << val_loss_accum_log << ","
            << lr << ","
            << norm << ","
            << (dt * 1000.0) << ","
            << tokens_per_sec << ","
            << (time_data * 1000.0) << ","
            << (time_forward * 1000.0) << ","
            << (time_loss * 1000.0) << ","
            << (time_backward * 1000.0) << ","
            << (time_clip * 1000.0) << ","
            << (time_optim * 1000.0) << ","
            << (model.t_tok_emb * 1000.0) << ","
            << (model.t_pos_emb * 1000.0) << ","
            << (model.t_mlp * 1000.0) << ","
            << (model.t_ln_f * 1000.0) << ","
            << (model.t_lm_head * 1000.0) << "\n";

            log_file.flush();
            model.reset_timing();
        }
        val_loss_accum_log = -1.0f;  // Reset for next iteration
    }
    if(rank == 0){
            log_file.close();
            std::cout << "\nTraining log saved to: " << log_filename << std::endl;
        
            std::cout << "\n=== Training Complete ===" << std::endl;
        }
        MPI_Finalize();
        return 0;
        
    } catch (const std::exception& e) {
        std::cerr << "ERROR: " << e.what() << __LINE__ << std::endl;
        return 1;
    }
}

```

## Verification

```
validation loss: 10.8480
validation loss: 10.7943
[DIAG] logits.requires_grad() = 1
[DIAG] logits.grad_fn() = ReshapeBackward
[DIAG] loss requires_grad = 1
[DIAG] loss grad_fn = SparseCrossEntropyLossBackward
step     0 | loss: 10.849461 | lr 6.1350e-07 | norm: 8.5342 | dt: 2919.94ms | tok/sec: 22444.28
```

> [!NOTE]
> [gpt2_test_new.cpp](file:///home/blu-bridge25/Study/Code/TensorParallelismBeta/DTensor/gpt2_test_new.cpp) (single-GPU) was never affected by bugs 1, 2, or 4 since those only manifest in multi-GPU with NCCL collectives and cross-device tensor management.
