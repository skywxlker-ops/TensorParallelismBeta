Got it! Here are optimizations within your DTensor / CustomDNN scope only (excluding Tensor-Implementations):

DTensor Layer (
tensor/
, process_group/)
Async AllReduce in ProcessGroupNCCL
Current: all_reduce is synchronous (blocks until complete).
Fix: Add all_reduce_async() that returns a handle, allowing overlap with next layer's compute.
Sharded Embedding Lookup (
dtensor.cpp
)
Current: 
DEmbedding
 is fully replicated on every GPU.
Fix: Shard the embedding table by vocabulary rows. Use AllGather to collect only the needed embeddings.
GPU-side Grad Norm (
dtensor.cpp
)
Current: 
compute_grad_norm
 in training scripts copies gradients to CPU.
Fix: Implement DTensor::grad_norm() that uses GPU reduction + all_reduce_cpu on a single float.
Reduce Unnecessary Syncs
Current: Multiple cudaDeviceSynchronize() calls in training loop.
Fix: Remove or minimize them; rely on CUDA stream ordering.
CustomDNN Layer (nn/)
Fused Forward in 
DMLP
Current: 
forward()
 calls fc1, 
gelu
, fc2 as separate operations.
Fix: Add a fused kernel path or batch the operations with a single all_reduce at the end instead of two.
Lazy Weight Init in 
DLinear
Current: Weights are initialized eagerly on construction.
Fix: Delay allocation until first 
forward()
, reducing peak memory during model setup.
Remove Redundant Copies in 
DLayerNorm
Review DLayerNorm::forward and 
backward
 for any unnecessary tensor copies.
DataLoader (data/)
Async Prefetching in 
DataLoaderLite
Current: 
next_batch()
 blocks while loading from disk.
Fix: Use a background thread to pre-load the next batch while the current one is training.
Priority Order (Impact vs. Effort):
Priority	Optimization	Impact
1	Async AllReduce	High
2	GPU-side Grad Norm	Medium
3	Sharded Embedding	Medium
4	Fused DMLP Forward	Medium
5	Async DataLoader Prefetch	Low-Medium
