flowchart TD
    subgraph Step 1: Setup on CPU
        A[Program Starts on N Processes] --> B{Rank 0 generates a ncclUniqueId};
        B --> C[Rank 0 broadcasts the ncclUniqueId to all other N-1 processes <br/>(This usually happens via MPI or a network socket)];
        D{Ranks 1 to N-1 receive the ncclUniqueId};
    end

    subgraph Step 2: NCCL Initialization on Each GPU
        C --> E;
        D --> E[All N processes now have the same Unique ID];
        E --> F{Each process calls its ProcessGroup constructor};
        F --> G["Inside constructor: ncclCommInitRank is called <br/> NCCL uses the ID to establish a communication ring between GPUs"];
    end

    subgraph Step 3: Perform a Collective Operation (e.g., AllReduce)
        G --> H{Application calls process_group->allReduce()};
        H --> I["Inside allReduce: ncclAllReduce() is called <br/> The operation is enqueued on the dedicated CUDA stream"];
        I --> J[NCCL executes the AllReduce operation <br/> using high-speed interconnects like NVLink];
        J --> K{The result is written to the destination buffer on all GPUs};
    end

    subgraph Step 4: Synchronization
        K --> L{The allReduce function returns a 'Work' object};
        L --> M{Application can continue doing other work on the GPU};
        M --> N{When the result is needed on the CPU, <br/> the application calls work->wait()};
        N --> O[The CPU thread blocks until the GPU <br/> finishes the AllReduce operation];
        O --> P[Execution Continues];
    end
    